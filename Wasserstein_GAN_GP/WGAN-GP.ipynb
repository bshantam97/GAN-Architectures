{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, datasets, transforms\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"C:\\\\Users\\\\shant\\\\celeba\"\n",
    "LEARNING_RATE = 0.00001\n",
    "BATCH_SIZE = 64\n",
    "Z_DIM = 100\n",
    "FEATURES_DISC = 64\n",
    "FEATURES_GEN = 64\n",
    "IMAGE_CHANNELS = 3\n",
    "NUM_EPOCHS = 5\n",
    "IMAGE_SIZE = 64\n",
    "CRITIC_ITERATIONS = 5\n",
    "LAMBDA_GP = 10\n",
    "\n",
    "dataset = datasets.ImageFolder(root = root , transform=transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.CenterCrop(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "]))\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the Wasserstein GAN\n",
    "# WGAN Has better stability and loss means something for WGAN: Its a termination criteria\n",
    "# WGAN Also prevents Mode Collapse(Model only outputs specific classes)\n",
    "# When Discriminator converged to 0 obtained great results\n",
    "\n",
    "class Generator(nn.Sequential):\n",
    "    \"\"\"\n",
    "    z_dim: \n",
    "    channels_img: Input channels(for example for an RGB image this value is 3)\n",
    "    features_g: Size of the output feature map(In this case its 64x64)\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, channels_img, features_g):\n",
    "        \n",
    "        modules = [self._block(z_dim, features_g*16, 4, 1, 0),\n",
    "                   self._block(features_g*16, features_g*8, 4, 2, 1),\n",
    "                   self._block(features_g*8, features_g*4, 4, 2, 1),\n",
    "                   self._block(features_g*4, features_g*2, 4, 2, 1),\n",
    "                   nn.ConvTranspose2d(features_g*2, channels_img, 4, 2, 1),\n",
    "                   nn.Tanh()]\n",
    "        \n",
    "        super(Generator, self).__init__(*modules)\n",
    "    \n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        \n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "class Critic(nn.Sequential):\n",
    "    \n",
    "    def __init__(self, channels_img, features_d):\n",
    "        \n",
    "        modules = [nn.Conv2d(channels_img, features_d, kernel_size = 4, stride = 2, padding = 1), #32x32\n",
    "                   nn.LeakyReLU(0.2, inplace=True),\n",
    "                   self._block(features_d, features_d*2, 4, 2, 1),# 16x16\n",
    "                   self._block(features_d*2, features_d*4, 4, 2, 1), #8x8\n",
    "                   self._block(features_d*4, features_d*8, 4, 2, 1), #4x4\n",
    "                   nn.Conv2d(features_d*8, 1, kernel_size = 4, stride = 2, padding = 0)]\n",
    "        \n",
    "        super(Critic, self).__init__(*modules)\n",
    "    \n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        \n",
    "        return nn.Sequential(\n",
    "         nn.Conv2d(in_channels, out_channels,kernel_size, stride, padding, bias=False),\n",
    "            nn.InstanceNorm2d(out_channels, affine = True), # Learnable Parameters\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    if classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "def gradient_penalty(critic, real, fake, device = \"cpu\"):\n",
    "    batch_size, C, H, W = real.shape\n",
    "    # Creating interpolated images\n",
    "    epsilon = torch.randn([batch_size, 1, 1, 1]).repeat(1,C,H,W).to(device)\n",
    "    interpolated_images = epsilon*real + (1-epsilon) * fake\n",
    "\n",
    "    #calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images)\n",
    "\n",
    "    # Compute the gradients with respect to the interpolated images, just need the first value\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs = interpolated_images, \n",
    "        outputs = mixed_scores, \n",
    "        grad_outputs = torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True)[0]\n",
    "\n",
    "    # Number of Dimension\n",
    "    # Calculate the Norm of the gradient to Eforce 1-Lipschitz Constraint\n",
    "    gradient = gradient.view(gradient.size(0), -1)\n",
    "    gradient_norm = gradient.norm(2, dim = 1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1)**2)\n",
    "\n",
    "    return gradient_penalty\n",
    "\n",
    "# def test():\n",
    "#     N, in_channels, H, W = 8, 3, 64, 64\n",
    "#     z_dim = 100\n",
    "#     X = torch.randn((N, in_channels, H, W))\n",
    "#     disc = Critic(in_channels,8)\n",
    "#     disc.apply(weights_init)\n",
    "#     assert disc(X).shape == (N, 1, 1, 1) # One Value per example\n",
    "#     gen = Generator(z_dim, in_channels, 64)\n",
    "#     gen.apply(weights_init)\n",
    "#     z = torch.randn((N, z_dim, 1, 1))\n",
    "#     assert gen(z).shape == (N, in_channels, H, W) # Ouput Generated image\n",
    "#     print(\"Success\")\n",
    "# test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Generator and Discriminator Model objects\n",
    "########################\n",
    "generator = Generator(Z_DIM,IMAGE_CHANNELS,FEATURES_GEN).to(device)\n",
    "critic = Critic(IMAGE_CHANNELS,FEATURES_DISC).to(device)\n",
    "\n",
    "########################\n",
    "# Weight Initialization for the model\n",
    "########################\n",
    "generator.apply(weights_init)\n",
    "critic.apply(weights_init)\n",
    "\n",
    "########################\n",
    "# Optimizers for Critic and the Generator\n",
    "########################\n",
    "optimizer_gen = optim.Adam(generator.parameters(), lr = LEARNING_RATE, betas = (0,0.9))\n",
    "optimizer_critic = optim.Adam(critic.parameters(), lr = LEARNING_RATE, betas = (0,0.9))\n",
    "\n",
    "#######################\n",
    "# Create tensorboard SummaryWriter objects to display generated fake images and associated loss curves\n",
    "#######################\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "loss_curves = SummaryWriter(f\"logs/loss_curves\")\n",
    "\n",
    "#######################\n",
    "# Create a batch of latent vectors. Will be used to to do a single pass through the generator after \n",
    "# the training has terminated\n",
    "#######################\n",
    "fixed_noise = torch.randn((64, Z_DIM, 1, 1)).to(device)\n",
    "\n",
    "step = 0 # For printing to tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/5] Batch 0/3166                   Loss D: -215.7560, loss G: -1.8182\n",
      "Epoch [0/5] Batch 50/3166                   Loss D: 29.8819, loss G: 23.8634\n",
      "Epoch [0/5] Batch 100/3166                   Loss D: 70.4569, loss G: 52.4091\n",
      "Epoch [0/5] Batch 150/3166                   Loss D: 86.0928, loss G: 64.4262\n",
      "Epoch [0/5] Batch 200/3166                   Loss D: 106.8502, loss G: 75.5788\n",
      "Epoch [0/5] Batch 250/3166                   Loss D: 103.8101, loss G: 79.9860\n",
      "Epoch [0/5] Batch 300/3166                   Loss D: 66.9414, loss G: 75.8155\n",
      "Epoch [0/5] Batch 350/3166                   Loss D: 78.5758, loss G: 70.8416\n",
      "Epoch [0/5] Batch 400/3166                   Loss D: 74.5777, loss G: 69.2469\n",
      "Epoch [0/5] Batch 450/3166                   Loss D: 57.6903, loss G: 64.7120\n",
      "Epoch [0/5] Batch 500/3166                   Loss D: 47.7621, loss G: 52.6940\n",
      "Epoch [0/5] Batch 550/3166                   Loss D: 42.5687, loss G: 42.6688\n",
      "Epoch [0/5] Batch 600/3166                   Loss D: 34.7462, loss G: 38.5643\n",
      "Epoch [0/5] Batch 650/3166                   Loss D: 25.1749, loss G: 32.7172\n",
      "Epoch [0/5] Batch 700/3166                   Loss D: 27.5956, loss G: 32.1347\n",
      "Epoch [0/5] Batch 750/3166                   Loss D: 23.5981, loss G: 33.8794\n",
      "Epoch [0/5] Batch 800/3166                   Loss D: 19.2411, loss G: 29.2170\n",
      "Epoch [0/5] Batch 850/3166                   Loss D: 18.8189, loss G: 31.7313\n",
      "Epoch [0/5] Batch 900/3166                   Loss D: 16.6535, loss G: 31.0509\n",
      "Epoch [0/5] Batch 950/3166                   Loss D: 13.8595, loss G: 31.9794\n",
      "Epoch [0/5] Batch 1000/3166                   Loss D: 17.7305, loss G: 29.3044\n",
      "Epoch [0/5] Batch 1050/3166                   Loss D: 14.9940, loss G: 28.6768\n",
      "Epoch [0/5] Batch 1100/3166                   Loss D: 12.7022, loss G: 30.6421\n",
      "Epoch [0/5] Batch 1150/3166                   Loss D: 14.0333, loss G: 28.0648\n",
      "Epoch [0/5] Batch 1200/3166                   Loss D: 12.3115, loss G: 26.4927\n",
      "Epoch [0/5] Batch 1250/3166                   Loss D: 14.2607, loss G: 28.8170\n",
      "Epoch [0/5] Batch 1300/3166                   Loss D: 14.3791, loss G: 26.0951\n",
      "Epoch [0/5] Batch 1350/3166                   Loss D: 15.2989, loss G: 30.4336\n",
      "Epoch [0/5] Batch 1400/3166                   Loss D: 13.9663, loss G: 28.8881\n",
      "Epoch [0/5] Batch 1450/3166                   Loss D: 13.2604, loss G: 31.8551\n",
      "Epoch [0/5] Batch 1500/3166                   Loss D: 12.7265, loss G: 31.5739\n",
      "Epoch [0/5] Batch 1550/3166                   Loss D: 13.8329, loss G: 30.0172\n",
      "Epoch [0/5] Batch 1600/3166                   Loss D: 14.3071, loss G: 33.3474\n",
      "Epoch [0/5] Batch 1650/3166                   Loss D: 11.8160, loss G: 34.9072\n",
      "Epoch [0/5] Batch 1700/3166                   Loss D: 12.3573, loss G: 35.6866\n",
      "Epoch [0/5] Batch 1750/3166                   Loss D: 13.2337, loss G: 38.8571\n",
      "Epoch [0/5] Batch 1800/3166                   Loss D: 13.4205, loss G: 38.2320\n",
      "Epoch [0/5] Batch 1850/3166                   Loss D: 13.6413, loss G: 36.6601\n",
      "Epoch [0/5] Batch 1900/3166                   Loss D: 13.8909, loss G: 38.1871\n",
      "Epoch [0/5] Batch 1950/3166                   Loss D: 16.7100, loss G: 39.8498\n",
      "Epoch [0/5] Batch 2000/3166                   Loss D: 13.8115, loss G: 40.2046\n",
      "Epoch [0/5] Batch 2050/3166                   Loss D: 14.9185, loss G: 43.8818\n",
      "Epoch [0/5] Batch 2100/3166                   Loss D: 12.5844, loss G: 41.0760\n",
      "Epoch [0/5] Batch 2150/3166                   Loss D: 16.1202, loss G: 43.1909\n",
      "Epoch [0/5] Batch 2200/3166                   Loss D: 15.9079, loss G: 46.8513\n",
      "Epoch [0/5] Batch 2250/3166                   Loss D: 12.7506, loss G: 43.4135\n",
      "Epoch [0/5] Batch 2300/3166                   Loss D: 14.1721, loss G: 45.4827\n",
      "Epoch [0/5] Batch 2350/3166                   Loss D: 13.4729, loss G: 48.2550\n",
      "Epoch [0/5] Batch 2400/3166                   Loss D: 14.4061, loss G: 45.2679\n",
      "Epoch [0/5] Batch 2450/3166                   Loss D: 15.2035, loss G: 46.2239\n",
      "Epoch [0/5] Batch 2500/3166                   Loss D: 17.3333, loss G: 48.1717\n",
      "Epoch [0/5] Batch 2550/3166                   Loss D: 14.4084, loss G: 46.4728\n",
      "Epoch [0/5] Batch 2600/3166                   Loss D: 14.4063, loss G: 50.2132\n",
      "Epoch [0/5] Batch 2650/3166                   Loss D: 15.9379, loss G: 48.6960\n",
      "Epoch [0/5] Batch 2700/3166                   Loss D: 12.9135, loss G: 45.5452\n",
      "Epoch [0/5] Batch 2750/3166                   Loss D: 16.8044, loss G: 50.6399\n",
      "Epoch [0/5] Batch 2800/3166                   Loss D: 15.0411, loss G: 49.1129\n",
      "Epoch [0/5] Batch 2850/3166                   Loss D: 19.1864, loss G: 49.5968\n",
      "Epoch [0/5] Batch 2900/3166                   Loss D: 15.8237, loss G: 49.3027\n",
      "Epoch [0/5] Batch 2950/3166                   Loss D: 14.2308, loss G: 48.4277\n",
      "Epoch [0/5] Batch 3000/3166                   Loss D: 13.3631, loss G: 50.6195\n",
      "Epoch [0/5] Batch 3050/3166                   Loss D: 14.2618, loss G: 50.1539\n",
      "Epoch [0/5] Batch 3100/3166                   Loss D: 12.1464, loss G: 47.8742\n",
      "Epoch [0/5] Batch 3150/3166                   Loss D: 15.4156, loss G: 48.6070\n",
      "Epoch [1/5] Batch 0/3166                   Loss D: 16.4353, loss G: 45.1145\n",
      "Epoch [1/5] Batch 50/3166                   Loss D: 13.8913, loss G: 48.5190\n",
      "Epoch [1/5] Batch 100/3166                   Loss D: 15.5869, loss G: 46.7969\n",
      "Epoch [1/5] Batch 150/3166                   Loss D: 15.9342, loss G: 50.0883\n",
      "Epoch [1/5] Batch 200/3166                   Loss D: 14.8746, loss G: 49.7440\n",
      "Epoch [1/5] Batch 250/3166                   Loss D: 15.0817, loss G: 49.8376\n",
      "Epoch [1/5] Batch 300/3166                   Loss D: 11.6972, loss G: 51.4341\n",
      "Epoch [1/5] Batch 350/3166                   Loss D: 13.1835, loss G: 50.4615\n",
      "Epoch [1/5] Batch 400/3166                   Loss D: 13.8862, loss G: 48.7424\n",
      "Epoch [1/5] Batch 450/3166                   Loss D: 15.3413, loss G: 49.3243\n",
      "Epoch [1/5] Batch 500/3166                   Loss D: 13.6956, loss G: 52.5437\n",
      "Epoch [1/5] Batch 550/3166                   Loss D: 11.9321, loss G: 52.5249\n",
      "Epoch [1/5] Batch 600/3166                   Loss D: 16.2714, loss G: 53.3366\n",
      "Epoch [1/5] Batch 650/3166                   Loss D: 17.1919, loss G: 52.7784\n",
      "Epoch [1/5] Batch 700/3166                   Loss D: 15.3625, loss G: 55.2504\n",
      "Epoch [1/5] Batch 750/3166                   Loss D: 14.8010, loss G: 52.6369\n",
      "Epoch [1/5] Batch 800/3166                   Loss D: 12.2495, loss G: 56.0539\n",
      "Epoch [1/5] Batch 850/3166                   Loss D: 16.1851, loss G: 50.9995\n",
      "Epoch [1/5] Batch 900/3166                   Loss D: 13.3523, loss G: 53.3082\n",
      "Epoch [1/5] Batch 950/3166                   Loss D: 11.6362, loss G: 52.4274\n",
      "Epoch [1/5] Batch 1000/3166                   Loss D: 12.9341, loss G: 54.4298\n",
      "Epoch [1/5] Batch 1050/3166                   Loss D: 12.9520, loss G: 52.7906\n",
      "Epoch [1/5] Batch 1100/3166                   Loss D: 11.7529, loss G: 56.9364\n",
      "Epoch [1/5] Batch 1150/3166                   Loss D: 13.2698, loss G: 55.6718\n",
      "Epoch [1/5] Batch 1200/3166                   Loss D: 11.8806, loss G: 52.0915\n",
      "Epoch [1/5] Batch 1250/3166                   Loss D: 12.3644, loss G: 58.4745\n",
      "Epoch [1/5] Batch 1300/3166                   Loss D: 15.6223, loss G: 56.2379\n",
      "Epoch [1/5] Batch 1350/3166                   Loss D: 11.0313, loss G: 55.2670\n",
      "Epoch [1/5] Batch 1400/3166                   Loss D: 13.5581, loss G: 60.8548\n",
      "Epoch [1/5] Batch 1450/3166                   Loss D: 15.3313, loss G: 58.4860\n",
      "Epoch [1/5] Batch 1500/3166                   Loss D: 14.8652, loss G: 58.4495\n",
      "Epoch [1/5] Batch 1550/3166                   Loss D: 15.4703, loss G: 55.6544\n",
      "Epoch [1/5] Batch 1600/3166                   Loss D: 12.8164, loss G: 60.5848\n",
      "Epoch [1/5] Batch 1650/3166                   Loss D: 14.6720, loss G: 61.3382\n",
      "Epoch [1/5] Batch 1700/3166                   Loss D: 14.3719, loss G: 59.8829\n",
      "Epoch [1/5] Batch 1750/3166                   Loss D: 15.4503, loss G: 58.6558\n",
      "Epoch [1/5] Batch 1800/3166                   Loss D: 14.4010, loss G: 63.1386\n",
      "Epoch [1/5] Batch 1850/3166                   Loss D: 12.8743, loss G: 59.9012\n",
      "Epoch [1/5] Batch 1900/3166                   Loss D: 12.3528, loss G: 63.4638\n",
      "Epoch [1/5] Batch 1950/3166                   Loss D: 14.3582, loss G: 63.9502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Batch 2000/3166                   Loss D: 14.9621, loss G: 63.4083\n",
      "Epoch [1/5] Batch 2050/3166                   Loss D: 14.5861, loss G: 62.8509\n",
      "Epoch [1/5] Batch 2100/3166                   Loss D: 13.8006, loss G: 64.3905\n",
      "Epoch [1/5] Batch 2150/3166                   Loss D: 13.9927, loss G: 62.7234\n",
      "Epoch [1/5] Batch 2200/3166                   Loss D: 15.1381, loss G: 63.9889\n",
      "Epoch [1/5] Batch 2250/3166                   Loss D: 11.6759, loss G: 65.4801\n",
      "Epoch [1/5] Batch 2300/3166                   Loss D: 14.0205, loss G: 62.5690\n",
      "Epoch [1/5] Batch 2350/3166                   Loss D: 13.1323, loss G: 70.3030\n",
      "Epoch [1/5] Batch 2400/3166                   Loss D: 14.3723, loss G: 67.4497\n",
      "Epoch [1/5] Batch 2450/3166                   Loss D: 14.0902, loss G: 66.4464\n",
      "Epoch [1/5] Batch 2500/3166                   Loss D: 12.4679, loss G: 66.7262\n",
      "Epoch [1/5] Batch 2550/3166                   Loss D: 14.3514, loss G: 67.9610\n",
      "Epoch [1/5] Batch 2600/3166                   Loss D: 11.3775, loss G: 66.7198\n",
      "Epoch [1/5] Batch 2650/3166                   Loss D: 13.5979, loss G: 64.8898\n",
      "Epoch [1/5] Batch 2700/3166                   Loss D: 12.0167, loss G: 70.4558\n",
      "Epoch [1/5] Batch 2750/3166                   Loss D: 10.5133, loss G: 69.3361\n",
      "Epoch [1/5] Batch 2800/3166                   Loss D: 12.4025, loss G: 67.7994\n",
      "Epoch [1/5] Batch 2850/3166                   Loss D: 12.8208, loss G: 70.1600\n",
      "Epoch [1/5] Batch 2900/3166                   Loss D: 13.2248, loss G: 71.6783\n",
      "Epoch [1/5] Batch 2950/3166                   Loss D: 14.4866, loss G: 70.8578\n",
      "Epoch [1/5] Batch 3000/3166                   Loss D: 13.3882, loss G: 68.5321\n",
      "Epoch [1/5] Batch 3050/3166                   Loss D: 13.0321, loss G: 61.9761\n",
      "Epoch [1/5] Batch 3100/3166                   Loss D: 11.6720, loss G: 71.8563\n",
      "Epoch [1/5] Batch 3150/3166                   Loss D: 15.7604, loss G: 68.5883\n",
      "Epoch [2/5] Batch 0/3166                   Loss D: 13.0440, loss G: 66.0980\n",
      "Epoch [2/5] Batch 50/3166                   Loss D: 12.4059, loss G: 70.6389\n",
      "Epoch [2/5] Batch 100/3166                   Loss D: 12.8738, loss G: 73.1919\n",
      "Epoch [2/5] Batch 150/3166                   Loss D: 15.1499, loss G: 64.6266\n",
      "Epoch [2/5] Batch 200/3166                   Loss D: 13.3288, loss G: 72.6546\n",
      "Epoch [2/5] Batch 250/3166                   Loss D: 13.7092, loss G: 72.9316\n",
      "Epoch [2/5] Batch 300/3166                   Loss D: 11.8611, loss G: 68.9961\n",
      "Epoch [2/5] Batch 350/3166                   Loss D: 14.4653, loss G: 71.2023\n",
      "Epoch [2/5] Batch 400/3166                   Loss D: 14.8344, loss G: 70.3558\n",
      "Epoch [2/5] Batch 450/3166                   Loss D: 11.6744, loss G: 72.2706\n",
      "Epoch [2/5] Batch 500/3166                   Loss D: 14.0715, loss G: 72.9300\n",
      "Epoch [2/5] Batch 550/3166                   Loss D: 12.3966, loss G: 70.9829\n",
      "Epoch [2/5] Batch 600/3166                   Loss D: 14.2935, loss G: 73.2533\n",
      "Epoch [2/5] Batch 650/3166                   Loss D: 13.5193, loss G: 70.1428\n",
      "Epoch [2/5] Batch 700/3166                   Loss D: 13.2703, loss G: 74.2090\n",
      "Epoch [2/5] Batch 750/3166                   Loss D: 12.7484, loss G: 73.4358\n",
      "Epoch [2/5] Batch 800/3166                   Loss D: 12.8390, loss G: 74.8571\n",
      "Epoch [2/5] Batch 850/3166                   Loss D: 12.5795, loss G: 73.1824\n",
      "Epoch [2/5] Batch 900/3166                   Loss D: 13.2062, loss G: 71.9018\n",
      "Epoch [2/5] Batch 950/3166                   Loss D: 12.9193, loss G: 74.7809\n",
      "Epoch [2/5] Batch 1000/3166                   Loss D: 15.4363, loss G: 72.4127\n",
      "Epoch [2/5] Batch 1050/3166                   Loss D: 12.3663, loss G: 72.7024\n",
      "Epoch [2/5] Batch 1100/3166                   Loss D: 13.7135, loss G: 74.3961\n",
      "Epoch [2/5] Batch 1150/3166                   Loss D: 11.9724, loss G: 73.9775\n",
      "Epoch [2/5] Batch 1200/3166                   Loss D: 14.2546, loss G: 75.8794\n",
      "Epoch [2/5] Batch 1250/3166                   Loss D: 12.0603, loss G: 73.9172\n",
      "Epoch [2/5] Batch 1300/3166                   Loss D: 12.1927, loss G: 75.2396\n",
      "Epoch [2/5] Batch 1350/3166                   Loss D: 12.7526, loss G: 74.4999\n",
      "Epoch [2/5] Batch 1400/3166                   Loss D: 10.9268, loss G: 76.1727\n",
      "Epoch [2/5] Batch 1450/3166                   Loss D: 13.8377, loss G: 76.0788\n",
      "Epoch [2/5] Batch 1500/3166                   Loss D: 13.5225, loss G: 79.4578\n",
      "Epoch [2/5] Batch 1550/3166                   Loss D: 13.1364, loss G: 76.8972\n",
      "Epoch [2/5] Batch 1600/3166                   Loss D: 11.5579, loss G: 79.0603\n",
      "Epoch [2/5] Batch 1650/3166                   Loss D: 12.9323, loss G: 76.9226\n",
      "Epoch [2/5] Batch 1700/3166                   Loss D: 10.9969, loss G: 77.1832\n",
      "Epoch [2/5] Batch 1750/3166                   Loss D: 13.5281, loss G: 78.7219\n",
      "Epoch [2/5] Batch 1800/3166                   Loss D: 15.8312, loss G: 74.5647\n",
      "Epoch [2/5] Batch 1850/3166                   Loss D: 12.1591, loss G: 74.3661\n",
      "Epoch [2/5] Batch 1900/3166                   Loss D: 13.0095, loss G: 81.3283\n",
      "Epoch [2/5] Batch 1950/3166                   Loss D: 10.4518, loss G: 76.4590\n",
      "Epoch [2/5] Batch 2000/3166                   Loss D: 12.9881, loss G: 75.2915\n",
      "Epoch [2/5] Batch 2050/3166                   Loss D: 11.7534, loss G: 74.2855\n",
      "Epoch [2/5] Batch 2100/3166                   Loss D: 10.4007, loss G: 76.9430\n",
      "Epoch [2/5] Batch 2150/3166                   Loss D: 13.1176, loss G: 77.3577\n",
      "Epoch [2/5] Batch 2200/3166                   Loss D: 13.1792, loss G: 78.8701\n",
      "Epoch [2/5] Batch 2250/3166                   Loss D: 14.1079, loss G: 77.9901\n",
      "Epoch [2/5] Batch 2300/3166                   Loss D: 11.4098, loss G: 78.7087\n",
      "Epoch [2/5] Batch 2350/3166                   Loss D: 11.7054, loss G: 76.3504\n",
      "Epoch [2/5] Batch 2400/3166                   Loss D: 11.4927, loss G: 78.5550\n",
      "Epoch [2/5] Batch 2450/3166                   Loss D: 12.9443, loss G: 76.6195\n",
      "Epoch [2/5] Batch 2500/3166                   Loss D: 12.7201, loss G: 83.0824\n",
      "Epoch [2/5] Batch 2550/3166                   Loss D: 13.2935, loss G: 80.1137\n",
      "Epoch [2/5] Batch 2600/3166                   Loss D: 13.7758, loss G: 83.3583\n",
      "Epoch [2/5] Batch 2650/3166                   Loss D: 11.4992, loss G: 82.4334\n",
      "Epoch [2/5] Batch 2700/3166                   Loss D: 12.5175, loss G: 80.4839\n",
      "Epoch [2/5] Batch 2750/3166                   Loss D: 13.0264, loss G: 79.6189\n",
      "Epoch [2/5] Batch 2800/3166                   Loss D: 12.2259, loss G: 82.3489\n",
      "Epoch [2/5] Batch 2850/3166                   Loss D: 13.5055, loss G: 81.0786\n",
      "Epoch [2/5] Batch 2900/3166                   Loss D: 14.1554, loss G: 84.0242\n",
      "Epoch [2/5] Batch 2950/3166                   Loss D: 14.7071, loss G: 84.8712\n",
      "Epoch [2/5] Batch 3000/3166                   Loss D: 12.6265, loss G: 83.3712\n",
      "Epoch [2/5] Batch 3050/3166                   Loss D: 13.4805, loss G: 82.7745\n",
      "Epoch [2/5] Batch 3100/3166                   Loss D: 10.6577, loss G: 86.2489\n",
      "Epoch [2/5] Batch 3150/3166                   Loss D: 14.6490, loss G: 82.2610\n",
      "Epoch [3/5] Batch 0/3166                   Loss D: 14.4463, loss G: 78.4605\n",
      "Epoch [3/5] Batch 50/3166                   Loss D: 11.0276, loss G: 79.0578\n",
      "Epoch [3/5] Batch 100/3166                   Loss D: 12.4080, loss G: 86.0034\n",
      "Epoch [3/5] Batch 150/3166                   Loss D: 12.0088, loss G: 83.3995\n",
      "Epoch [3/5] Batch 200/3166                   Loss D: 11.7387, loss G: 82.6915\n",
      "Epoch [3/5] Batch 250/3166                   Loss D: 14.7403, loss G: 80.2286\n",
      "Epoch [3/5] Batch 300/3166                   Loss D: 12.5280, loss G: 85.3241\n",
      "Epoch [3/5] Batch 350/3166                   Loss D: 12.2635, loss G: 79.7577\n",
      "Epoch [3/5] Batch 400/3166                   Loss D: 13.7686, loss G: 83.2724\n",
      "Epoch [3/5] Batch 450/3166                   Loss D: 14.1336, loss G: 83.9918\n",
      "Epoch [3/5] Batch 500/3166                   Loss D: 11.3388, loss G: 84.0723\n",
      "Epoch [3/5] Batch 550/3166                   Loss D: 12.2974, loss G: 83.0117\n",
      "Epoch [3/5] Batch 600/3166                   Loss D: 13.5200, loss G: 81.7071\n",
      "Epoch [3/5] Batch 650/3166                   Loss D: 12.4484, loss G: 82.7241\n",
      "Epoch [3/5] Batch 700/3166                   Loss D: 13.5047, loss G: 84.0765\n",
      "Epoch [3/5] Batch 750/3166                   Loss D: 13.2085, loss G: 84.5838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5] Batch 800/3166                   Loss D: 11.7285, loss G: 82.5719\n",
      "Epoch [3/5] Batch 850/3166                   Loss D: 10.8758, loss G: 83.8583\n",
      "Epoch [3/5] Batch 900/3166                   Loss D: 13.5974, loss G: 83.3039\n",
      "Epoch [3/5] Batch 950/3166                   Loss D: 11.4693, loss G: 86.2271\n",
      "Epoch [3/5] Batch 1000/3166                   Loss D: 11.6608, loss G: 85.6967\n",
      "Epoch [3/5] Batch 1050/3166                   Loss D: 11.5968, loss G: 84.3653\n",
      "Epoch [3/5] Batch 1100/3166                   Loss D: 11.3065, loss G: 85.7727\n",
      "Epoch [3/5] Batch 1150/3166                   Loss D: 11.9035, loss G: 85.1466\n",
      "Epoch [3/5] Batch 1200/3166                   Loss D: 12.3141, loss G: 82.8839\n",
      "Epoch [3/5] Batch 1250/3166                   Loss D: 11.9012, loss G: 83.6463\n",
      "Epoch [3/5] Batch 1300/3166                   Loss D: 11.8969, loss G: 85.9413\n",
      "Epoch [3/5] Batch 1350/3166                   Loss D: 11.6255, loss G: 84.9797\n",
      "Epoch [3/5] Batch 1400/3166                   Loss D: 10.3211, loss G: 87.4391\n",
      "Epoch [3/5] Batch 1450/3166                   Loss D: 14.2701, loss G: 85.0849\n",
      "Epoch [3/5] Batch 1500/3166                   Loss D: 12.3495, loss G: 84.1664\n",
      "Epoch [3/5] Batch 1550/3166                   Loss D: 13.0707, loss G: 87.8311\n",
      "Epoch [3/5] Batch 1600/3166                   Loss D: 10.9076, loss G: 86.2181\n",
      "Epoch [3/5] Batch 1650/3166                   Loss D: 13.7105, loss G: 87.4926\n",
      "Epoch [3/5] Batch 1700/3166                   Loss D: 11.7448, loss G: 89.3781\n",
      "Epoch [3/5] Batch 1750/3166                   Loss D: 10.7813, loss G: 84.8239\n",
      "Epoch [3/5] Batch 1800/3166                   Loss D: 15.9230, loss G: 87.9285\n",
      "Epoch [3/5] Batch 1850/3166                   Loss D: 11.2548, loss G: 86.4688\n",
      "Epoch [3/5] Batch 1900/3166                   Loss D: 12.3790, loss G: 89.7780\n",
      "Epoch [3/5] Batch 1950/3166                   Loss D: 12.3546, loss G: 87.1280\n",
      "Epoch [3/5] Batch 2000/3166                   Loss D: 10.4990, loss G: 84.2725\n",
      "Epoch [3/5] Batch 2050/3166                   Loss D: 14.6355, loss G: 86.2148\n",
      "Epoch [3/5] Batch 2100/3166                   Loss D: 12.8868, loss G: 87.7545\n",
      "Epoch [3/5] Batch 2150/3166                   Loss D: 12.1523, loss G: 88.9814\n",
      "Epoch [3/5] Batch 2200/3166                   Loss D: 14.1067, loss G: 86.1283\n",
      "Epoch [3/5] Batch 2250/3166                   Loss D: 10.5828, loss G: 90.9198\n",
      "Epoch [3/5] Batch 2300/3166                   Loss D: 12.3338, loss G: 88.4515\n",
      "Epoch [3/5] Batch 2350/3166                   Loss D: 12.9039, loss G: 85.8716\n",
      "Epoch [3/5] Batch 2400/3166                   Loss D: 12.0402, loss G: 89.0214\n",
      "Epoch [3/5] Batch 2450/3166                   Loss D: 10.8695, loss G: 86.9588\n",
      "Epoch [3/5] Batch 2500/3166                   Loss D: 11.5113, loss G: 89.5794\n",
      "Epoch [3/5] Batch 2550/3166                   Loss D: 10.7857, loss G: 89.6212\n",
      "Epoch [3/5] Batch 2600/3166                   Loss D: 13.2217, loss G: 93.1229\n",
      "Epoch [3/5] Batch 2650/3166                   Loss D: 12.7176, loss G: 93.4289\n",
      "Epoch [3/5] Batch 2700/3166                   Loss D: 9.1754, loss G: 90.0182\n",
      "Epoch [3/5] Batch 2750/3166                   Loss D: 10.4614, loss G: 88.6520\n",
      "Epoch [3/5] Batch 2800/3166                   Loss D: 11.0579, loss G: 87.4116\n",
      "Epoch [3/5] Batch 2850/3166                   Loss D: 10.7127, loss G: 93.2559\n",
      "Epoch [3/5] Batch 2900/3166                   Loss D: 11.7613, loss G: 88.0109\n",
      "Epoch [3/5] Batch 2950/3166                   Loss D: 10.1382, loss G: 89.3653\n",
      "Epoch [3/5] Batch 3000/3166                   Loss D: 10.4631, loss G: 90.6535\n",
      "Epoch [3/5] Batch 3050/3166                   Loss D: 11.8264, loss G: 91.5073\n",
      "Epoch [3/5] Batch 3100/3166                   Loss D: 9.6487, loss G: 88.9240\n",
      "Epoch [3/5] Batch 3150/3166                   Loss D: 8.0891, loss G: 88.5615\n",
      "Epoch [4/5] Batch 0/3166                   Loss D: 10.5148, loss G: 94.3982\n",
      "Epoch [4/5] Batch 50/3166                   Loss D: 12.7811, loss G: 92.6487\n",
      "Epoch [4/5] Batch 100/3166                   Loss D: 14.2718, loss G: 87.4046\n",
      "Epoch [4/5] Batch 150/3166                   Loss D: 10.9455, loss G: 94.0639\n",
      "Epoch [4/5] Batch 200/3166                   Loss D: 12.9994, loss G: 90.7221\n",
      "Epoch [4/5] Batch 250/3166                   Loss D: 10.5780, loss G: 87.9065\n",
      "Epoch [4/5] Batch 300/3166                   Loss D: 11.3424, loss G: 90.2787\n",
      "Epoch [4/5] Batch 350/3166                   Loss D: 11.1355, loss G: 94.4918\n",
      "Epoch [4/5] Batch 400/3166                   Loss D: 10.3115, loss G: 91.3080\n",
      "Epoch [4/5] Batch 450/3166                   Loss D: 10.8716, loss G: 96.5590\n",
      "Epoch [4/5] Batch 500/3166                   Loss D: 12.5342, loss G: 90.0423\n",
      "Epoch [4/5] Batch 550/3166                   Loss D: 14.9891, loss G: 92.7391\n",
      "Epoch [4/5] Batch 600/3166                   Loss D: 11.9473, loss G: 93.6328\n",
      "Epoch [4/5] Batch 650/3166                   Loss D: 10.8390, loss G: 88.6409\n",
      "Epoch [4/5] Batch 700/3166                   Loss D: 9.6970, loss G: 93.3134\n",
      "Epoch [4/5] Batch 750/3166                   Loss D: 10.4678, loss G: 91.1068\n",
      "Epoch [4/5] Batch 800/3166                   Loss D: 10.5926, loss G: 94.4094\n",
      "Epoch [4/5] Batch 850/3166                   Loss D: 10.7314, loss G: 87.7075\n",
      "Epoch [4/5] Batch 900/3166                   Loss D: 10.8175, loss G: 91.7858\n",
      "Epoch [4/5] Batch 950/3166                   Loss D: 9.9206, loss G: 91.4953\n",
      "Epoch [4/5] Batch 1000/3166                   Loss D: 10.4259, loss G: 96.4373\n",
      "Epoch [4/5] Batch 1050/3166                   Loss D: 10.7856, loss G: 93.8275\n",
      "Epoch [4/5] Batch 1100/3166                   Loss D: 9.8126, loss G: 92.9617\n",
      "Epoch [4/5] Batch 1150/3166                   Loss D: 12.1498, loss G: 93.4442\n",
      "Epoch [4/5] Batch 1200/3166                   Loss D: 11.9987, loss G: 93.9003\n",
      "Epoch [4/5] Batch 1250/3166                   Loss D: 9.5499, loss G: 93.0156\n",
      "Epoch [4/5] Batch 1300/3166                   Loss D: 12.6861, loss G: 96.9010\n",
      "Epoch [4/5] Batch 1350/3166                   Loss D: 9.3498, loss G: 95.2590\n",
      "Epoch [4/5] Batch 1400/3166                   Loss D: 12.2137, loss G: 94.6502\n",
      "Epoch [4/5] Batch 1450/3166                   Loss D: 8.0046, loss G: 93.9591\n",
      "Epoch [4/5] Batch 1500/3166                   Loss D: 10.6763, loss G: 93.4722\n",
      "Epoch [4/5] Batch 1550/3166                   Loss D: 9.6978, loss G: 92.2240\n",
      "Epoch [4/5] Batch 1600/3166                   Loss D: 10.5530, loss G: 93.4609\n",
      "Epoch [4/5] Batch 1650/3166                   Loss D: 11.3689, loss G: 91.9684\n",
      "Epoch [4/5] Batch 1700/3166                   Loss D: 11.4344, loss G: 91.9428\n",
      "Epoch [4/5] Batch 1750/3166                   Loss D: 8.5137, loss G: 95.6259\n",
      "Epoch [4/5] Batch 1800/3166                   Loss D: 9.2571, loss G: 94.3804\n",
      "Epoch [4/5] Batch 1850/3166                   Loss D: 10.1522, loss G: 93.7206\n",
      "Epoch [4/5] Batch 1900/3166                   Loss D: 9.8841, loss G: 96.6027\n",
      "Epoch [4/5] Batch 1950/3166                   Loss D: 10.0659, loss G: 96.8726\n",
      "Epoch [4/5] Batch 2000/3166                   Loss D: 11.2729, loss G: 99.7338\n",
      "Epoch [4/5] Batch 2050/3166                   Loss D: 11.4745, loss G: 93.7716\n",
      "Epoch [4/5] Batch 2100/3166                   Loss D: 10.3678, loss G: 93.4191\n",
      "Epoch [4/5] Batch 2150/3166                   Loss D: 10.2332, loss G: 98.2199\n",
      "Epoch [4/5] Batch 2200/3166                   Loss D: 9.5557, loss G: 93.3362\n",
      "Epoch [4/5] Batch 2250/3166                   Loss D: 10.5715, loss G: 96.9079\n",
      "Epoch [4/5] Batch 2300/3166                   Loss D: 11.3271, loss G: 95.5446\n",
      "Epoch [4/5] Batch 2350/3166                   Loss D: 11.6649, loss G: 97.8811\n",
      "Epoch [4/5] Batch 2400/3166                   Loss D: 12.4841, loss G: 92.9034\n",
      "Epoch [4/5] Batch 2450/3166                   Loss D: 10.2694, loss G: 98.6821\n",
      "Epoch [4/5] Batch 2500/3166                   Loss D: 11.1087, loss G: 96.8992\n",
      "Epoch [4/5] Batch 2550/3166                   Loss D: 12.6187, loss G: 98.4396\n",
      "Epoch [4/5] Batch 2600/3166                   Loss D: 11.1054, loss G: 93.3748\n",
      "Epoch [4/5] Batch 2650/3166                   Loss D: 11.4570, loss G: 96.3485\n",
      "Epoch [4/5] Batch 2700/3166                   Loss D: 9.9522, loss G: 98.5135\n",
      "Epoch [4/5] Batch 2750/3166                   Loss D: 11.4965, loss G: 93.1893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5] Batch 2800/3166                   Loss D: 9.5222, loss G: 97.1319\n",
      "Epoch [4/5] Batch 2850/3166                   Loss D: 10.8799, loss G: 95.9715\n",
      "Epoch [4/5] Batch 2900/3166                   Loss D: 12.7303, loss G: 97.8696\n",
      "Epoch [4/5] Batch 2950/3166                   Loss D: 11.6509, loss G: 97.9898\n",
      "Epoch [4/5] Batch 3000/3166                   Loss D: 8.6593, loss G: 98.6911\n",
      "Epoch [4/5] Batch 3050/3166                   Loss D: 10.3881, loss G: 94.9838\n",
      "Epoch [4/5] Batch 3100/3166                   Loss D: 10.9699, loss G: 93.7298\n",
      "Epoch [4/5] Batch 3150/3166                   Loss D: 8.9403, loss G: 99.4433\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    # Unsupervised\n",
    "    for batch_idx, (real, _) in enumerate(dataloader):\n",
    "        \n",
    "        # The real world images\n",
    "        real = real.to(device)\n",
    "        \n",
    "        cur_batch_size = real.shape[0]\n",
    "        #####################################################\n",
    "        # Train the Critic\n",
    "        #####################################################\n",
    "        \n",
    "        for _ in range(CRITIC_ITERATIONS):\n",
    "            critic.zero_grad()\n",
    "            # Latent noise\n",
    "            noise = torch.randn((cur_batch_size, Z_DIM, 1, 1)).to(device)\n",
    "            # Pass the latent vector through the generator\n",
    "            fake = generator(noise)     \n",
    "            critic_real = critic(real).view(-1)\n",
    "            critic_fake = critic(fake).view(-1)\n",
    "            gp = gradient_penalty(critic, real, fake, device=device)\n",
    "            ## Loss for the critic. Taking -ve because RMSProp are designed to minimize \n",
    "            ## Hence to minimize something -ve is equivalent to maximizing that expression\n",
    "            loss_critic = -torch.mean(critic_real) + torch.mean(critic_fake) + LAMBDA_GP*gp \n",
    "            loss_critic.backward(retain_graph=True)\n",
    "            optimizer_critic.step()\n",
    "\n",
    "        #############################\n",
    "        # Train the generator minimizing -E[critic(gen_fake)]\n",
    "        #############################\n",
    "        generator.zero_grad()\n",
    "        output = critic(fake).view(-1)\n",
    "        loss_gen = -torch.mean(output)\n",
    "        loss_gen.backward()\n",
    "        optimizer_gen.step()\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            \n",
    "            print(\n",
    "            f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "                  Loss D: {-loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                fake = generator(fixed_noise)\n",
    "            \n",
    "                # The [:64] prints out the 4-D tensor BxCxHxW\n",
    "                img_grid_real = torchvision.utils.make_grid(\n",
    "                    real[:64], normalize = True)\n",
    "                img_grid_fake = torchvision.utils.make_grid(\n",
    "                    fake[:64], normalize = True)\n",
    "                ##########################\n",
    "                # TensorBoard Visualizations\n",
    "                ##########################\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "#                 loss_curves.add_scalar(\"generator\", {loss_gen, global_step=step)\n",
    "                loss_curves.add_scalars(\"curves\", {\n",
    "                    \"generator\":loss_gen, \"critic\":-(loss_critic)\n",
    "                }, global_step = step)\n",
    "#                 loss_curves.add_scalar(\"discriminator\", loss_disc, global_step = step)\n",
    "                \n",
    "            step += 1 # See progression of images\n",
    "\n",
    "# Save critic and generator state dictionaries\n",
    "torch.save(generator.state_dict(), 'generator.pt')\n",
    "torch.save(critic.state_dict(), 'critic.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
