{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import models, datasets, transforms\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"C:\\\\Users\\\\shant\\\\celeba\"\n",
    "LEARNING_RATE = 0.00001\n",
    "BATCH_SIZE = 64\n",
    "Z_DIM = 100\n",
    "FEATURES_DISC = 16\n",
    "FEATURES_GEN = 16\n",
    "IMAGE_CHANNELS = 1\n",
    "NUM_EPOCHS = 20\n",
    "IMAGE_SIZE = 64\n",
    "CRITIC_ITERATIONS = 5\n",
    "LAMBDA_GP = 10\n",
    "NUM_CLASSES = 10\n",
    "GEN_EMBEDDING = 100\n",
    "\n",
    "transforms = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5 for _ in range(IMAGE_CHANNELS)], [0.5 for _ in range(IMAGE_CHANNELS)])\n",
    "])\n",
    "\n",
    "dataset = datasets.MNIST(root = \"../dataset\", transform = transforms, download = True)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    z_dim: \n",
    "    channels_img: Input channels(for example for an RGB image this value is 3)\n",
    "    features_g: Size of the output feature map(In this case its 64x64)\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, channels_img, features_g, num_classes, img_size, embed_size):\n",
    "        \n",
    "        super(Generator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.embed  = nn.Embedding(num_classes, embed_size)\n",
    "        self.gen = nn.Sequential(self._block(z_dim + embed_size, features_g*16, 4, 1, 0),\n",
    "                                 self._block(features_g*16, features_g*8, 4, 2, 1),\n",
    "                                 self._block(features_g*8, features_g*4, 4, 2, 1),\n",
    "                                 self._block(features_g*4, features_g*2, 4, 2, 1),\n",
    "                                 nn.ConvTranspose2d(features_g*2, channels_img, 4, 2, 1),\n",
    "                                 nn.Tanh())\n",
    "    \n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        \n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        # latent vector z: N x noise_dim x 1 x 1\n",
    "        embedding = self.embed(labels).unsqueeze(2).unsqueeze(3) # Basically adds dimension 1 and then again dimension 1\n",
    "        x = torch.cat([x,embedding], dim = 1)\n",
    "        return self.gen(x)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self, channels_img, features_d, num_classes, img_size):\n",
    "        \n",
    "        super(Critic, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.embed = nn.Embedding(num_classes, img_size*img_size)\n",
    "        \n",
    "        ## Additional stamp as to what the image is \n",
    "        self.critic = nn.Sequential(nn.Conv2d(channels_img + 1, features_d, kernel_size = 4, stride = 2, padding = 1), #32x32\n",
    "                                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                                    self._block(features_d, features_d*2, 4, 2, 1),# 16x16\n",
    "                                    self._block(features_d*2, features_d*4, 4, 2, 1), #8x8\n",
    "                                    self._block(features_d*4, features_d*8, 4, 2, 1), #4x4\n",
    "                                    nn.Conv2d(features_d*8, 1, kernel_size = 4, stride = 2, padding = 0)\n",
    "                                    )\n",
    "    \n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        \n",
    "        return nn.Sequential(\n",
    "         nn.Conv2d(in_channels, out_channels,kernel_size, stride, padding, bias=False),\n",
    "            nn.InstanceNorm2d(out_channels, affine = True),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x,labels):\n",
    "        embedding = self.embed(labels).view(labels.shape[0], 1, self.img_size, self.img_size)\n",
    "        x = torch.cat([x,embedding], dim = 1) # N x C x H x W\n",
    "        return self.critic(x)\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    if classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "def gradient_penalty(critic, real, fake, labels, device = \"cpu\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    # Creating interpolated images\n",
    "    epsilon = torch.randn([BATCH_SIZE, 1, 1, 1]).repeat(1,C,H,W).to(device)\n",
    "    interpolated_images = real*epsilon + fake * (1-epsilon)\n",
    "\n",
    "    #calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images, labels)\n",
    "\n",
    "    # Compute the gradients with respect to the interpolated images, just need the first value\n",
    "    gradient = torch.autograd.grad(inputs = interpolated_images, \n",
    "    outputs = mixed_scores, \n",
    "    grad_outputs = torch.ones_like(mixed_scores),\n",
    "    create_graph=True,\n",
    "    retain_graph=True)[0]\n",
    "\n",
    "    # Number of Dimension\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim = 1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1)**2)\n",
    "\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Generator and Discriminator Model objects\n",
    "########################\n",
    "generator = Generator(Z_DIM,IMAGE_CHANNELS,FEATURES_GEN,NUM_CLASSES,IMAGE_SIZE,GEN_EMBEDDING).to(device)\n",
    "critic = Critic(IMAGE_CHANNELS,FEATURES_DISC,NUM_CLASSES,IMAGE_SIZE).to(device)\n",
    "\n",
    "########################\n",
    "# Weight Initialization for the model\n",
    "########################\n",
    "generator.apply(weights_init)\n",
    "critic.apply(weights_init)\n",
    "\n",
    "########################\n",
    "# Optimizers for Critic and the Generator\n",
    "########################\n",
    "optimizer_gen = optim.Adam(generator.parameters(), lr = LEARNING_RATE, betas = (0,0.9))\n",
    "optimizer_critic = optim.Adam(critic.parameters(), lr = LEARNING_RATE, betas = (0,0.9))\n",
    "\n",
    "#######################\n",
    "# Create tensorboard SummaryWriter objects to display generated fake images and associated loss curves\n",
    "#######################\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "loss_curves = SummaryWriter(f\"logs/loss_curves\")\n",
    "\n",
    "#######################\n",
    "# Create a batch of latent vectors. Will be used to to do a single pass through the generator after \n",
    "# the training has terminated\n",
    "#######################\n",
    "fixed_noise = torch.randn((64, Z_DIM, 1, 1)).to(device)\n",
    "\n",
    "step = 0 # For printing to tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/20] Batch 0/938                   Loss D: 0.5922, loss G: 0.2442\n",
      "Epoch [0/20] Batch 100/938                   Loss D: -20.8695, loss G: 13.0117\n",
      "Epoch [0/20] Batch 200/938                   Loss D: -35.6456, loss G: 22.8758\n",
      "Epoch [0/20] Batch 300/938                   Loss D: -48.7674, loss G: 30.9714\n",
      "Epoch [0/20] Batch 400/938                   Loss D: -64.0156, loss G: 37.2053\n",
      "Epoch [0/20] Batch 500/938                   Loss D: -68.3097, loss G: 42.2915\n",
      "Epoch [0/20] Batch 600/938                   Loss D: -68.6418, loss G: 46.4080\n",
      "Epoch [0/20] Batch 700/938                   Loss D: -71.6571, loss G: 48.1020\n",
      "Epoch [0/20] Batch 800/938                   Loss D: -76.8946, loss G: 48.4690\n",
      "Epoch [0/20] Batch 900/938                   Loss D: -68.7592, loss G: 49.4654\n",
      "Epoch [1/20] Batch 0/938                   Loss D: -66.8409, loss G: 50.1887\n",
      "Epoch [1/20] Batch 100/938                   Loss D: -66.4959, loss G: 51.8003\n",
      "Epoch [1/20] Batch 200/938                   Loss D: -64.3293, loss G: 48.7724\n",
      "Epoch [1/20] Batch 300/938                   Loss D: -64.7446, loss G: 48.3654\n",
      "Epoch [1/20] Batch 400/938                   Loss D: -59.6180, loss G: 49.3700\n",
      "Epoch [1/20] Batch 500/938                   Loss D: -53.8567, loss G: 51.0174\n",
      "Epoch [1/20] Batch 600/938                   Loss D: -53.1833, loss G: 52.7737\n",
      "Epoch [1/20] Batch 700/938                   Loss D: -40.6556, loss G: 51.2146\n",
      "Epoch [1/20] Batch 800/938                   Loss D: -38.9306, loss G: 52.7257\n",
      "Epoch [1/20] Batch 900/938                   Loss D: -43.2880, loss G: 54.0464\n",
      "Epoch [2/20] Batch 0/938                   Loss D: -41.4224, loss G: 54.9104\n",
      "Epoch [2/20] Batch 100/938                   Loss D: -36.8397, loss G: 53.6413\n",
      "Epoch [2/20] Batch 200/938                   Loss D: -25.8223, loss G: 55.3527\n",
      "Epoch [2/20] Batch 300/938                   Loss D: -30.6831, loss G: 53.6573\n",
      "Epoch [2/20] Batch 400/938                   Loss D: -32.8170, loss G: 53.3532\n",
      "Epoch [2/20] Batch 500/938                   Loss D: -28.3303, loss G: 52.7926\n",
      "Epoch [2/20] Batch 600/938                   Loss D: -23.0811, loss G: 51.8686\n",
      "Epoch [2/20] Batch 700/938                   Loss D: -18.6772, loss G: 50.6351\n",
      "Epoch [2/20] Batch 800/938                   Loss D: -19.1733, loss G: 47.9138\n",
      "Epoch [2/20] Batch 900/938                   Loss D: -22.8798, loss G: 46.4440\n",
      "Epoch [3/20] Batch 0/938                   Loss D: -20.7118, loss G: 46.3908\n",
      "Epoch [3/20] Batch 100/938                   Loss D: -19.1689, loss G: 44.2535\n",
      "Epoch [3/20] Batch 200/938                   Loss D: -15.0253, loss G: 43.7668\n",
      "Epoch [3/20] Batch 300/938                   Loss D: -17.4306, loss G: 42.4341\n",
      "Epoch [3/20] Batch 400/938                   Loss D: -19.7219, loss G: 42.2397\n",
      "Epoch [3/20] Batch 500/938                   Loss D: -16.9118, loss G: 40.2128\n",
      "Epoch [3/20] Batch 600/938                   Loss D: -14.8235, loss G: 39.7744\n",
      "Epoch [3/20] Batch 700/938                   Loss D: -12.4260, loss G: 39.4897\n",
      "Epoch [3/20] Batch 800/938                   Loss D: -13.3624, loss G: 37.7719\n",
      "Epoch [3/20] Batch 900/938                   Loss D: -12.8555, loss G: 38.8272\n",
      "Epoch [4/20] Batch 0/938                   Loss D: -13.5339, loss G: 39.2853\n",
      "Epoch [4/20] Batch 100/938                   Loss D: -14.9602, loss G: 38.9647\n",
      "Epoch [4/20] Batch 200/938                   Loss D: -11.2262, loss G: 38.2632\n",
      "Epoch [4/20] Batch 300/938                   Loss D: -12.3389, loss G: 37.4995\n",
      "Epoch [4/20] Batch 400/938                   Loss D: -12.2228, loss G: 38.6524\n",
      "Epoch [4/20] Batch 500/938                   Loss D: -13.3534, loss G: 39.5474\n",
      "Epoch [4/20] Batch 600/938                   Loss D: -12.2418, loss G: 40.4681\n",
      "Epoch [4/20] Batch 700/938                   Loss D: -13.3666, loss G: 41.4360\n",
      "Epoch [4/20] Batch 800/938                   Loss D: -12.9960, loss G: 41.0594\n",
      "Epoch [4/20] Batch 900/938                   Loss D: -12.1906, loss G: 41.4972\n",
      "Epoch [5/20] Batch 0/938                   Loss D: -12.8885, loss G: 40.4361\n",
      "Epoch [5/20] Batch 100/938                   Loss D: -11.5111, loss G: 40.7006\n",
      "Epoch [5/20] Batch 200/938                   Loss D: -12.0737, loss G: 41.3078\n",
      "Epoch [5/20] Batch 300/938                   Loss D: -12.4446, loss G: 42.0146\n",
      "Epoch [5/20] Batch 400/938                   Loss D: -10.4022, loss G: 40.9483\n",
      "Epoch [5/20] Batch 500/938                   Loss D: -10.8309, loss G: 41.7897\n",
      "Epoch [5/20] Batch 600/938                   Loss D: -12.7986, loss G: 41.5950\n",
      "Epoch [5/20] Batch 700/938                   Loss D: -10.9153, loss G: 43.1049\n",
      "Epoch [5/20] Batch 800/938                   Loss D: -10.2768, loss G: 45.1332\n",
      "Epoch [5/20] Batch 900/938                   Loss D: -11.4805, loss G: 44.6321\n",
      "Epoch [6/20] Batch 0/938                   Loss D: -10.2666, loss G: 44.2363\n",
      "Epoch [6/20] Batch 100/938                   Loss D: -9.0939, loss G: 45.3724\n",
      "Epoch [6/20] Batch 200/938                   Loss D: -11.1605, loss G: 42.3010\n",
      "Epoch [6/20] Batch 300/938                   Loss D: -9.1417, loss G: 44.7280\n",
      "Epoch [6/20] Batch 400/938                   Loss D: -10.9614, loss G: 44.2744\n",
      "Epoch [6/20] Batch 500/938                   Loss D: -9.4478, loss G: 43.6612\n",
      "Epoch [6/20] Batch 600/938                   Loss D: -9.0430, loss G: 45.8683\n",
      "Epoch [6/20] Batch 700/938                   Loss D: -10.7142, loss G: 45.3081\n",
      "Epoch [6/20] Batch 800/938                   Loss D: -8.6678, loss G: 45.8810\n",
      "Epoch [6/20] Batch 900/938                   Loss D: -10.1956, loss G: 46.0082\n",
      "Epoch [7/20] Batch 0/938                   Loss D: -8.2821, loss G: 44.3185\n",
      "Epoch [7/20] Batch 100/938                   Loss D: -9.2880, loss G: 45.6140\n",
      "Epoch [7/20] Batch 200/938                   Loss D: -8.7485, loss G: 45.7614\n",
      "Epoch [7/20] Batch 300/938                   Loss D: -10.3507, loss G: 45.4624\n",
      "Epoch [7/20] Batch 400/938                   Loss D: -7.5417, loss G: 46.6280\n",
      "Epoch [7/20] Batch 500/938                   Loss D: -8.6957, loss G: 45.9320\n",
      "Epoch [7/20] Batch 600/938                   Loss D: -8.2061, loss G: 45.4672\n",
      "Epoch [7/20] Batch 700/938                   Loss D: -7.7510, loss G: 45.9361\n",
      "Epoch [7/20] Batch 800/938                   Loss D: -6.6445, loss G: 46.6327\n",
      "Epoch [7/20] Batch 900/938                   Loss D: -8.1881, loss G: 46.5362\n",
      "Epoch [8/20] Batch 0/938                   Loss D: -6.6778, loss G: 46.9256\n",
      "Epoch [8/20] Batch 100/938                   Loss D: -6.1985, loss G: 45.2604\n",
      "Epoch [8/20] Batch 200/938                   Loss D: -7.9345, loss G: 47.4676\n",
      "Epoch [8/20] Batch 300/938                   Loss D: -6.4392, loss G: 48.3327\n",
      "Epoch [8/20] Batch 400/938                   Loss D: -6.3303, loss G: 48.2926\n",
      "Epoch [8/20] Batch 500/938                   Loss D: -6.6624, loss G: 46.9154\n",
      "Epoch [8/20] Batch 600/938                   Loss D: -6.9384, loss G: 46.6239\n",
      "Epoch [8/20] Batch 700/938                   Loss D: -6.2573, loss G: 47.5331\n",
      "Epoch [8/20] Batch 800/938                   Loss D: -5.6797, loss G: 49.7508\n",
      "Epoch [8/20] Batch 900/938                   Loss D: -7.2558, loss G: 47.0400\n",
      "Epoch [9/20] Batch 0/938                   Loss D: -7.4975, loss G: 46.1512\n",
      "Epoch [9/20] Batch 100/938                   Loss D: -5.4482, loss G: 46.6445\n",
      "Epoch [9/20] Batch 200/938                   Loss D: -5.9160, loss G: 48.5193\n",
      "Epoch [9/20] Batch 300/938                   Loss D: -5.8848, loss G: 47.8920\n",
      "Epoch [9/20] Batch 400/938                   Loss D: -6.3080, loss G: 46.7273\n",
      "Epoch [9/20] Batch 500/938                   Loss D: -5.5020, loss G: 48.1429\n",
      "Epoch [9/20] Batch 600/938                   Loss D: -6.0100, loss G: 48.3825\n",
      "Epoch [9/20] Batch 700/938                   Loss D: -4.5708, loss G: 49.4520\n",
      "Epoch [9/20] Batch 800/938                   Loss D: -5.8274, loss G: 48.6218\n",
      "Epoch [9/20] Batch 900/938                   Loss D: -5.8066, loss G: 48.1884\n",
      "Epoch [10/20] Batch 0/938                   Loss D: -6.4314, loss G: 48.6428\n",
      "Epoch [10/20] Batch 100/938                   Loss D: -4.9903, loss G: 47.7321\n",
      "Epoch [10/20] Batch 200/938                   Loss D: -4.3419, loss G: 46.7785\n",
      "Epoch [10/20] Batch 300/938                   Loss D: -5.8010, loss G: 49.9793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20] Batch 400/938                   Loss D: -6.2532, loss G: 50.5627\n",
      "Epoch [10/20] Batch 500/938                   Loss D: -3.8411, loss G: 48.2113\n",
      "Epoch [10/20] Batch 600/938                   Loss D: -4.5510, loss G: 49.9797\n",
      "Epoch [10/20] Batch 700/938                   Loss D: -5.4373, loss G: 50.2165\n",
      "Epoch [10/20] Batch 800/938                   Loss D: -5.3137, loss G: 49.9549\n",
      "Epoch [10/20] Batch 900/938                   Loss D: -4.9965, loss G: 50.6844\n",
      "Epoch [11/20] Batch 0/938                   Loss D: -6.7556, loss G: 50.1444\n",
      "Epoch [11/20] Batch 100/938                   Loss D: -5.0263, loss G: 50.4235\n",
      "Epoch [11/20] Batch 200/938                   Loss D: -4.8432, loss G: 48.7346\n",
      "Epoch [11/20] Batch 300/938                   Loss D: -6.1418, loss G: 50.3876\n",
      "Epoch [11/20] Batch 400/938                   Loss D: -5.3442, loss G: 50.1099\n",
      "Epoch [11/20] Batch 500/938                   Loss D: -5.2292, loss G: 48.5527\n",
      "Epoch [11/20] Batch 600/938                   Loss D: -6.4843, loss G: 48.6272\n",
      "Epoch [11/20] Batch 700/938                   Loss D: -5.1249, loss G: 49.3743\n",
      "Epoch [11/20] Batch 800/938                   Loss D: -5.2118, loss G: 50.1850\n",
      "Epoch [11/20] Batch 900/938                   Loss D: -5.7600, loss G: 50.5029\n",
      "Epoch [12/20] Batch 0/938                   Loss D: -4.7853, loss G: 48.7414\n",
      "Epoch [12/20] Batch 100/938                   Loss D: -5.2261, loss G: 50.1424\n",
      "Epoch [12/20] Batch 200/938                   Loss D: -5.6580, loss G: 50.9982\n",
      "Epoch [12/20] Batch 300/938                   Loss D: -4.5934, loss G: 50.0831\n",
      "Epoch [12/20] Batch 400/938                   Loss D: -4.3175, loss G: 49.3923\n",
      "Epoch [12/20] Batch 500/938                   Loss D: -4.1122, loss G: 50.8193\n",
      "Epoch [12/20] Batch 600/938                   Loss D: -4.0671, loss G: 50.6526\n",
      "Epoch [12/20] Batch 700/938                   Loss D: -5.5072, loss G: 50.9529\n",
      "Epoch [12/20] Batch 800/938                   Loss D: -5.3182, loss G: 51.2057\n",
      "Epoch [12/20] Batch 900/938                   Loss D: -4.5766, loss G: 49.8691\n",
      "Epoch [13/20] Batch 0/938                   Loss D: -4.4897, loss G: 50.7521\n",
      "Epoch [13/20] Batch 100/938                   Loss D: -5.4543, loss G: 50.4557\n",
      "Epoch [13/20] Batch 200/938                   Loss D: -3.9786, loss G: 51.1662\n",
      "Epoch [13/20] Batch 300/938                   Loss D: -4.8501, loss G: 51.6535\n",
      "Epoch [13/20] Batch 400/938                   Loss D: -4.6459, loss G: 51.5205\n",
      "Epoch [13/20] Batch 500/938                   Loss D: -4.2415, loss G: 52.5664\n",
      "Epoch [13/20] Batch 600/938                   Loss D: -3.6876, loss G: 50.3700\n",
      "Epoch [13/20] Batch 700/938                   Loss D: -4.4110, loss G: 50.7034\n",
      "Epoch [13/20] Batch 800/938                   Loss D: -5.3632, loss G: 52.2088\n",
      "Epoch [13/20] Batch 900/938                   Loss D: -4.6777, loss G: 51.5068\n",
      "Epoch [14/20] Batch 0/938                   Loss D: -4.9947, loss G: 50.3870\n",
      "Epoch [14/20] Batch 100/938                   Loss D: -3.9556, loss G: 51.7119\n",
      "Epoch [14/20] Batch 200/938                   Loss D: -3.9276, loss G: 51.3661\n",
      "Epoch [14/20] Batch 300/938                   Loss D: -4.6630, loss G: 51.8270\n",
      "Epoch [14/20] Batch 400/938                   Loss D: -4.1811, loss G: 51.1162\n",
      "Epoch [14/20] Batch 500/938                   Loss D: -4.7998, loss G: 51.9027\n",
      "Epoch [14/20] Batch 600/938                   Loss D: -4.7516, loss G: 51.3317\n",
      "Epoch [14/20] Batch 700/938                   Loss D: -3.7222, loss G: 50.9726\n",
      "Epoch [14/20] Batch 800/938                   Loss D: -3.6651, loss G: 50.8092\n",
      "Epoch [14/20] Batch 900/938                   Loss D: -3.6566, loss G: 52.3288\n",
      "Epoch [15/20] Batch 0/938                   Loss D: -3.8254, loss G: 52.7983\n",
      "Epoch [15/20] Batch 100/938                   Loss D: -4.1253, loss G: 52.3091\n",
      "Epoch [15/20] Batch 200/938                   Loss D: -3.3054, loss G: 52.3695\n",
      "Epoch [15/20] Batch 300/938                   Loss D: -2.9954, loss G: 52.5555\n",
      "Epoch [15/20] Batch 400/938                   Loss D: -3.1470, loss G: 51.8645\n",
      "Epoch [15/20] Batch 500/938                   Loss D: -2.9789, loss G: 50.2388\n",
      "Epoch [15/20] Batch 600/938                   Loss D: -3.8105, loss G: 51.5518\n",
      "Epoch [15/20] Batch 700/938                   Loss D: -3.7854, loss G: 53.1920\n",
      "Epoch [15/20] Batch 800/938                   Loss D: -3.5348, loss G: 52.6928\n",
      "Epoch [15/20] Batch 900/938                   Loss D: -4.3459, loss G: 51.1843\n",
      "Epoch [16/20] Batch 0/938                   Loss D: -3.2335, loss G: 51.3177\n",
      "Epoch [16/20] Batch 100/938                   Loss D: -3.2855, loss G: 51.1082\n",
      "Epoch [16/20] Batch 200/938                   Loss D: -4.6761, loss G: 53.0205\n",
      "Epoch [16/20] Batch 300/938                   Loss D: -4.7226, loss G: 50.6048\n",
      "Epoch [16/20] Batch 400/938                   Loss D: -4.8384, loss G: 51.0722\n",
      "Epoch [16/20] Batch 500/938                   Loss D: -3.5515, loss G: 51.4843\n",
      "Epoch [16/20] Batch 600/938                   Loss D: -3.3196, loss G: 51.9547\n",
      "Epoch [16/20] Batch 700/938                   Loss D: -3.3476, loss G: 52.4943\n",
      "Epoch [16/20] Batch 800/938                   Loss D: -4.2083, loss G: 51.6358\n",
      "Epoch [16/20] Batch 900/938                   Loss D: -3.3705, loss G: 53.1151\n",
      "Epoch [17/20] Batch 0/938                   Loss D: -3.8307, loss G: 51.8433\n",
      "Epoch [17/20] Batch 100/938                   Loss D: -3.7018, loss G: 54.3934\n",
      "Epoch [17/20] Batch 200/938                   Loss D: -3.7995, loss G: 51.6352\n",
      "Epoch [17/20] Batch 300/938                   Loss D: -3.5362, loss G: 52.9121\n",
      "Epoch [17/20] Batch 400/938                   Loss D: -3.1779, loss G: 52.3707\n",
      "Epoch [17/20] Batch 500/938                   Loss D: -4.9222, loss G: 50.3368\n",
      "Epoch [17/20] Batch 600/938                   Loss D: -3.7514, loss G: 52.4309\n",
      "Epoch [17/20] Batch 700/938                   Loss D: -3.4739, loss G: 52.6711\n",
      "Epoch [17/20] Batch 800/938                   Loss D: -2.7537, loss G: 52.6083\n",
      "Epoch [17/20] Batch 900/938                   Loss D: -4.2942, loss G: 51.9585\n",
      "Epoch [18/20] Batch 0/938                   Loss D: -3.8162, loss G: 53.2284\n",
      "Epoch [18/20] Batch 100/938                   Loss D: -3.2172, loss G: 52.6511\n",
      "Epoch [18/20] Batch 200/938                   Loss D: -4.8355, loss G: 51.7069\n",
      "Epoch [18/20] Batch 300/938                   Loss D: -3.4365, loss G: 52.9919\n",
      "Epoch [18/20] Batch 400/938                   Loss D: -3.0559, loss G: 54.3919\n",
      "Epoch [18/20] Batch 500/938                   Loss D: -3.4705, loss G: 53.3096\n",
      "Epoch [18/20] Batch 600/938                   Loss D: -2.6907, loss G: 53.5989\n",
      "Epoch [18/20] Batch 700/938                   Loss D: -2.9342, loss G: 51.4868\n",
      "Epoch [18/20] Batch 800/938                   Loss D: -3.4928, loss G: 51.4410\n",
      "Epoch [18/20] Batch 900/938                   Loss D: -3.6695, loss G: 52.4283\n",
      "Epoch [19/20] Batch 0/938                   Loss D: -3.4947, loss G: 53.0570\n",
      "Epoch [19/20] Batch 100/938                   Loss D: -3.2919, loss G: 51.7656\n",
      "Epoch [19/20] Batch 200/938                   Loss D: -4.2176, loss G: 51.2611\n",
      "Epoch [19/20] Batch 300/938                   Loss D: -3.5691, loss G: 53.7127\n",
      "Epoch [19/20] Batch 400/938                   Loss D: -3.4047, loss G: 53.5502\n",
      "Epoch [19/20] Batch 500/938                   Loss D: -3.5976, loss G: 51.5696\n",
      "Epoch [19/20] Batch 600/938                   Loss D: -3.6393, loss G: 52.1459\n",
      "Epoch [19/20] Batch 700/938                   Loss D: -3.7204, loss G: 52.6311\n",
      "Epoch [19/20] Batch 800/938                   Loss D: -2.9645, loss G: 52.6787\n",
      "Epoch [19/20] Batch 900/938                   Loss D: -3.1378, loss G: 53.0012\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    # Unsupervised\n",
    "    for batch_idx, (real, labels) in enumerate(dataloader):\n",
    "        \n",
    "        # The real world images\n",
    "        real = real.to(device)\n",
    "        \n",
    "        cur_batch_size = real.shape[0]\n",
    "        \n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #####################################################\n",
    "        # Train the Critic\n",
    "        #####################################################\n",
    "        \n",
    "        for _ in range(CRITIC_ITERATIONS):\n",
    "            critic.zero_grad()\n",
    "            # Latent noise\n",
    "            noise = torch.randn((cur_batch_size, Z_DIM, 1, 1)).to(device)\n",
    "            # Pass the latent vector through the generator\n",
    "            fake = generator(noise, labels)     \n",
    "            critic_real = critic(real, labels).view(-1)\n",
    "            critic_fake = critic(fake.detach(), labels).view(-1)\n",
    "            gp = gradient_penalty(critic, real, fake, labels, device=device)\n",
    "            ## Loss for the critic. Taking -ve because RMSProp are designed to minimize \n",
    "            ## Hence to minimize something -ve is equivalent to maximizing that expression\n",
    "            loss_critic = (-(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP*gp) \n",
    "            loss_critic.backward(retain_graph=True)\n",
    "            optimizer_critic.step()\n",
    "\n",
    "        #############################\n",
    "        # Train the generator minimizing -E[critic(gen_fake)]\n",
    "        #############################\n",
    "        generator.zero_grad()\n",
    "        output = critic(fake, labels).view(-1)\n",
    "        loss_gen = -torch.mean(output)\n",
    "        loss_gen.backward()\n",
    "        optimizer_gen.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            \n",
    "            print(\n",
    "            f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "                  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                fake = generator(noise, labels)\n",
    "            \n",
    "                # The [:64] prints out the 4-D tensor BxCxHxW\n",
    "                img_grid_real = torchvision.utils.make_grid(\n",
    "                    real[:64], normalize = True)\n",
    "                img_grid_fake = torchvision.utils.make_grid(\n",
    "                    fake[:64], normalize = True)\n",
    "                ##########################\n",
    "                # TensorBoard Visualizations\n",
    "                ##########################\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "#                 loss_curves.add_scalar(\"generator\", {loss_gen, global_step=step)\n",
    "                loss_curves.add_scalars(\"curves\", {\n",
    "                    \"generator\":loss_gen, \"critic\":loss_critic\n",
    "                }, global_step = step)\n",
    "#                 loss_curves.add_scalar(\"discriminator\", loss_disc, global_step = step)\n",
    "                \n",
    "            step += 1 # See progression of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
