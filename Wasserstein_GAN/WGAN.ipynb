{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, datasets, transforms\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"C:\\\\Users\\\\shant\\\\celeba\"\n",
    "LEARNING_RATE = 0.00005\n",
    "BATCH_SIZE = 64\n",
    "Z_DIM = 100\n",
    "FEATURES_DISC = 64\n",
    "FEATURES_GEN = 64\n",
    "IMAGE_CHANNELS = 3\n",
    "NUM_EPOCHS = 5\n",
    "IMAGE_SIZE = 64\n",
    "CRITIC_ITERATIONS = 5\n",
    "WEIGHT_CLIP = 0.01\n",
    "\n",
    "dataset = datasets.ImageFolder(root = root , transform=transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.CenterCrop(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "]))\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the Wasserstein GAN\n",
    "# WGAN Has better stability and loss means something for WGAN: Its a termination criteria\n",
    "# WGAN Also prevents Mode Collapse(Model only outputs specific classes)\n",
    "# When Discriminator converged to 0 obtained great results\n",
    "\n",
    "class Generator(nn.Sequential):\n",
    "    \"\"\"\n",
    "    z_dim: \n",
    "    channels_img: Input channels(for example for an RGB image this value is 3)\n",
    "    features_g: Size of the output feature map(In this case its 64x64)\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, channels_img, features_g):\n",
    "        \n",
    "        modules = [self._block(z_dim, features_g*16, 4, 1, 0),\n",
    "                   self._block(features_g*16, features_g*8, 4, 2, 1),\n",
    "                   self._block(features_g*8, features_g*4, 4, 2, 1),\n",
    "                   self._block(features_g*4, features_g*2, 4, 2, 1),\n",
    "                   nn.ConvTranspose2d(features_g*2, channels_img, 4, 2, 1),\n",
    "                   nn.Tanh()]\n",
    "        \n",
    "        super(Generator, self).__init__(*modules)\n",
    "    \n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        \n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "class Critic(nn.Sequential):\n",
    "    \n",
    "    def __init__(self, channels_img, features_d):\n",
    "        \n",
    "        modules = [nn.Conv2d(channels_img, features_d, kernel_size = 4, stride = 2, padding = 1), #32x32\n",
    "                   nn.LeakyReLU(0.2, inplace=True),\n",
    "                   self._block(features_d, features_d*2, 4, 2, 1),# 16x16\n",
    "                   self._block(features_d*2, features_d*4, 4, 2, 1), #8x8\n",
    "                   self._block(features_d*4, features_d*8, 4, 2, 1), #4x4\n",
    "                   nn.Conv2d(features_d*8, 1, kernel_size = 4, stride = 2, padding = 0)]\n",
    "        \n",
    "        super(Critic, self).__init__(*modules)\n",
    "    \n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        \n",
    "        return nn.Sequential(\n",
    "         nn.Conv2d(in_channels, out_channels,kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    if classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "            \n",
    "# def test():\n",
    "#     N, in_channels, H, W = 8, 3, 64, 64\n",
    "#     z_dim = 100\n",
    "#     X = torch.randn((N, in_channels, H, W))\n",
    "#     disc = Critic(in_channels,8)\n",
    "#     disc.apply(weights_init)\n",
    "#     assert disc(X).shape == (N, 1, 1, 1) # One Value per example\n",
    "#     gen = Generator(z_dim, in_channels, 64)\n",
    "#     gen.apply(weights_init)\n",
    "#     z = torch.randn((N, z_dim, 1, 1))\n",
    "#     assert gen(z).shape == (N, in_channels, H, W) # Ouput Generated image\n",
    "#     print(\"Success\")\n",
    "# test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Generator and Discriminator Model objects\n",
    "########################\n",
    "generator = Generator(Z_DIM,IMAGE_CHANNELS,FEATURES_GEN).to(device)\n",
    "critic = Critic(IMAGE_CHANNELS,FEATURES_DISC).to(device)\n",
    "\n",
    "########################\n",
    "# Weight Initialization for the model\n",
    "########################\n",
    "generator.apply(weights_init)\n",
    "critic.apply(weights_init)\n",
    "\n",
    "########################\n",
    "# Optimizers for Critic and the Generator\n",
    "########################\n",
    "optimizer_gen = optim.RMSprop(generator.parameters(), lr = LEARNING_RATE)\n",
    "optimizer_critic = optim.RMSprop(critic.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "#######################\n",
    "# Create tensorboard SummaryWriter objects to display generated fake images and associated loss curves\n",
    "#######################\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "loss_curves = SummaryWriter(f\"logs/loss_curves\")\n",
    "\n",
    "#######################\n",
    "# Create a batch of latent vectors. Will be used to to do a single pass through the generator after \n",
    "# the training has terminated\n",
    "#######################\n",
    "fixed_noise = torch.randn((64, Z_DIM, 1, 1)).to(device)\n",
    "\n",
    "step = 0 # For printing to tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/5] Batch 0/3166                   Loss D: -0.0363, loss C: 0.0255\n",
      "Epoch [0/5] Batch 100/3166                   Loss D: -0.6515, loss C: 0.2561\n",
      "Epoch [0/5] Batch 200/3166                   Loss D: -0.9469, loss C: 0.5037\n",
      "Epoch [0/5] Batch 300/3166                   Loss D: -1.1372, loss C: 0.5829\n",
      "Epoch [0/5] Batch 400/3166                   Loss D: -1.1746, loss C: 0.6057\n",
      "Epoch [0/5] Batch 500/3166                   Loss D: -1.0870, loss C: 0.6149\n",
      "Epoch [0/5] Batch 600/3166                   Loss D: -1.0651, loss C: 0.6028\n",
      "Epoch [0/5] Batch 700/3166                   Loss D: -1.0308, loss C: 0.6472\n",
      "Epoch [0/5] Batch 800/3166                   Loss D: -1.0858, loss C: 0.6396\n",
      "Epoch [0/5] Batch 900/3166                   Loss D: -0.6366, loss C: 0.4222\n",
      "Epoch [0/5] Batch 1000/3166                   Loss D: -1.0465, loss C: 0.5506\n",
      "Epoch [0/5] Batch 1100/3166                   Loss D: -0.7600, loss C: -0.1611\n",
      "Epoch [0/5] Batch 1200/3166                   Loss D: -0.9984, loss C: 0.4625\n",
      "Epoch [0/5] Batch 1300/3166                   Loss D: -1.0780, loss C: 0.5340\n",
      "Epoch [0/5] Batch 1400/3166                   Loss D: -0.9550, loss C: 0.3311\n",
      "Epoch [0/5] Batch 1500/3166                   Loss D: -1.0767, loss C: 0.5204\n",
      "Epoch [0/5] Batch 1600/3166                   Loss D: -1.0422, loss C: 0.6184\n",
      "Epoch [0/5] Batch 1700/3166                   Loss D: -0.6191, loss C: 0.6201\n",
      "Epoch [0/5] Batch 1800/3166                   Loss D: -1.1361, loss C: 0.6200\n",
      "Epoch [0/5] Batch 1900/3166                   Loss D: -1.0803, loss C: 0.5367\n",
      "Epoch [0/5] Batch 2000/3166                   Loss D: -0.6910, loss C: 0.3242\n",
      "Epoch [0/5] Batch 2100/3166                   Loss D: -0.9558, loss C: 0.4670\n",
      "Epoch [0/5] Batch 2200/3166                   Loss D: -0.9081, loss C: 0.3140\n",
      "Epoch [0/5] Batch 2300/3166                   Loss D: -1.0633, loss C: 0.5074\n",
      "Epoch [0/5] Batch 2400/3166                   Loss D: -0.8653, loss C: 0.6435\n",
      "Epoch [0/5] Batch 2500/3166                   Loss D: -1.0831, loss C: 0.5647\n",
      "Epoch [0/5] Batch 2600/3166                   Loss D: -0.9306, loss C: 0.4430\n",
      "Epoch [0/5] Batch 2700/3166                   Loss D: -1.1178, loss C: 0.6020\n",
      "Epoch [0/5] Batch 2800/3166                   Loss D: -0.9479, loss C: 0.3901\n",
      "Epoch [0/5] Batch 2900/3166                   Loss D: -0.9223, loss C: 0.3805\n",
      "Epoch [0/5] Batch 3000/3166                   Loss D: -0.9517, loss C: 0.6231\n",
      "Epoch [0/5] Batch 3100/3166                   Loss D: -1.0493, loss C: 0.5785\n",
      "Epoch [1/5] Batch 0/3166                   Loss D: -0.9196, loss C: 0.4329\n",
      "Epoch [1/5] Batch 100/3166                   Loss D: -0.9793, loss C: 0.5892\n",
      "Epoch [1/5] Batch 200/3166                   Loss D: -0.7510, loss C: 0.6212\n",
      "Epoch [1/5] Batch 300/3166                   Loss D: -0.7097, loss C: 0.5983\n",
      "Epoch [1/5] Batch 400/3166                   Loss D: -0.9904, loss C: 0.6259\n",
      "Epoch [1/5] Batch 500/3166                   Loss D: -0.8487, loss C: 0.5981\n",
      "Epoch [1/5] Batch 600/3166                   Loss D: -0.7839, loss C: 0.0834\n",
      "Epoch [1/5] Batch 700/3166                   Loss D: -0.8375, loss C: 0.3333\n",
      "Epoch [1/5] Batch 800/3166                   Loss D: -0.8651, loss C: 0.4959\n",
      "Epoch [1/5] Batch 900/3166                   Loss D: -0.9030, loss C: 0.4201\n",
      "Epoch [1/5] Batch 1000/3166                   Loss D: -0.7411, loss C: 0.1335\n",
      "Epoch [1/5] Batch 1100/3166                   Loss D: -0.8598, loss C: 0.3698\n",
      "Epoch [1/5] Batch 1200/3166                   Loss D: -0.9301, loss C: 0.4338\n",
      "Epoch [1/5] Batch 1300/3166                   Loss D: -0.7979, loss C: 0.5744\n",
      "Epoch [1/5] Batch 1400/3166                   Loss D: -0.7980, loss C: 0.2986\n",
      "Epoch [1/5] Batch 1500/3166                   Loss D: -0.8801, loss C: 0.5845\n",
      "Epoch [1/5] Batch 1600/3166                   Loss D: -0.8413, loss C: 0.5953\n",
      "Epoch [1/5] Batch 1700/3166                   Loss D: -0.7616, loss C: 0.3221\n",
      "Epoch [1/5] Batch 1800/3166                   Loss D: -0.8856, loss C: 0.4269\n",
      "Epoch [1/5] Batch 1900/3166                   Loss D: -0.8456, loss C: 0.5686\n",
      "Epoch [1/5] Batch 2000/3166                   Loss D: -0.7284, loss C: 0.2837\n",
      "Epoch [1/5] Batch 2100/3166                   Loss D: -0.7808, loss C: 0.5766\n",
      "Epoch [1/5] Batch 2200/3166                   Loss D: -0.7499, loss C: 0.1943\n",
      "Epoch [1/5] Batch 2300/3166                   Loss D: -0.8330, loss C: 0.3436\n",
      "Epoch [1/5] Batch 2400/3166                   Loss D: -0.7793, loss C: 0.5974\n",
      "Epoch [1/5] Batch 2500/3166                   Loss D: -0.8300, loss C: 0.5463\n",
      "Epoch [1/5] Batch 2600/3166                   Loss D: -0.6570, loss C: 0.2820\n",
      "Epoch [1/5] Batch 2700/3166                   Loss D: -0.8270, loss C: 0.5533\n",
      "Epoch [1/5] Batch 2800/3166                   Loss D: -0.7504, loss C: 0.5748\n",
      "Epoch [1/5] Batch 2900/3166                   Loss D: -0.7284, loss C: 0.2016\n",
      "Epoch [1/5] Batch 3000/3166                   Loss D: -0.8023, loss C: 0.5834\n",
      "Epoch [1/5] Batch 3100/3166                   Loss D: -0.7654, loss C: 0.5732\n",
      "Epoch [2/5] Batch 0/3166                   Loss D: -0.7408, loss C: 0.5200\n",
      "Epoch [2/5] Batch 100/3166                   Loss D: -0.8031, loss C: 0.5747\n",
      "Epoch [2/5] Batch 200/3166                   Loss D: -0.7333, loss C: 0.5742\n",
      "Epoch [2/5] Batch 300/3166                   Loss D: -0.7598, loss C: 0.5410\n",
      "Epoch [2/5] Batch 400/3166                   Loss D: -0.8481, loss C: 0.5624\n",
      "Epoch [2/5] Batch 500/3166                   Loss D: -0.7323, loss C: 0.2509\n",
      "Epoch [2/5] Batch 600/3166                   Loss D: -0.7707, loss C: 0.5706\n",
      "Epoch [2/5] Batch 700/3166                   Loss D: -0.8107, loss C: 0.5395\n",
      "Epoch [2/5] Batch 800/3166                   Loss D: -0.6920, loss C: 0.1563\n",
      "Epoch [2/5] Batch 900/3166                   Loss D: -0.6328, loss C: 0.2057\n",
      "Epoch [2/5] Batch 1000/3166                   Loss D: -0.7826, loss C: 0.2679\n",
      "Epoch [2/5] Batch 1100/3166                   Loss D: -0.7311, loss C: 0.2493\n",
      "Epoch [2/5] Batch 1200/3166                   Loss D: -0.7912, loss C: 0.3721\n",
      "Epoch [2/5] Batch 1300/3166                   Loss D: -0.6673, loss C: 0.5211\n",
      "Epoch [2/5] Batch 1400/3166                   Loss D: -0.6661, loss C: 0.1167\n",
      "Epoch [2/5] Batch 1500/3166                   Loss D: -0.6499, loss C: 0.1660\n",
      "Epoch [2/5] Batch 1600/3166                   Loss D: -0.6477, loss C: 0.0863\n",
      "Epoch [2/5] Batch 1700/3166                   Loss D: -0.7615, loss C: 0.5341\n",
      "Epoch [2/5] Batch 1800/3166                   Loss D: -0.7573, loss C: 0.4917\n",
      "Epoch [2/5] Batch 1900/3166                   Loss D: -0.6762, loss C: 0.2845\n",
      "Epoch [2/5] Batch 2000/3166                   Loss D: -0.7110, loss C: 0.2249\n",
      "Epoch [2/5] Batch 2100/3166                   Loss D: -0.8338, loss C: 0.4073\n",
      "Epoch [2/5] Batch 2200/3166                   Loss D: -0.6305, loss C: 0.5505\n",
      "Epoch [2/5] Batch 2300/3166                   Loss D: -0.7499, loss C: 0.5402\n",
      "Epoch [2/5] Batch 2400/3166                   Loss D: -0.6219, loss C: 0.5809\n",
      "Epoch [2/5] Batch 2500/3166                   Loss D: -0.6809, loss C: 0.2351\n",
      "Epoch [2/5] Batch 2600/3166                   Loss D: -0.6770, loss C: 0.2710\n",
      "Epoch [2/5] Batch 2700/3166                   Loss D: -0.6756, loss C: 0.2602\n",
      "Epoch [2/5] Batch 2800/3166                   Loss D: -0.7803, loss C: 0.3293\n",
      "Epoch [2/5] Batch 2900/3166                   Loss D: -0.6242, loss C: 0.2215\n",
      "Epoch [2/5] Batch 3000/3166                   Loss D: -0.7753, loss C: 0.4744\n",
      "Epoch [2/5] Batch 3100/3166                   Loss D: -0.6911, loss C: 0.5533\n",
      "Epoch [3/5] Batch 0/3166                   Loss D: -0.5973, loss C: 0.1875\n",
      "Epoch [3/5] Batch 100/3166                   Loss D: -0.7134, loss C: 0.5416\n",
      "Epoch [3/5] Batch 200/3166                   Loss D: -0.6259, loss C: 0.2266\n",
      "Epoch [3/5] Batch 300/3166                   Loss D: -0.6562, loss C: 0.2411\n",
      "Epoch [3/5] Batch 400/3166                   Loss D: -0.6920, loss C: 0.2081\n",
      "Epoch [3/5] Batch 500/3166                   Loss D: -0.6724, loss C: 0.5173\n",
      "Epoch [3/5] Batch 600/3166                   Loss D: -0.7302, loss C: 0.5294\n",
      "Epoch [3/5] Batch 700/3166                   Loss D: -0.6191, loss C: 0.1576\n",
      "Epoch [3/5] Batch 800/3166                   Loss D: -0.6625, loss C: 0.5283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5] Batch 900/3166                   Loss D: -0.5962, loss C: 0.1173\n",
      "Epoch [3/5] Batch 1000/3166                   Loss D: -0.7243, loss C: 0.5324\n",
      "Epoch [3/5] Batch 1100/3166                   Loss D: -0.6480, loss C: 0.2647\n",
      "Epoch [3/5] Batch 1200/3166                   Loss D: -0.5618, loss C: 0.5284\n",
      "Epoch [3/5] Batch 1300/3166                   Loss D: -0.5867, loss C: 0.0859\n",
      "Epoch [3/5] Batch 1400/3166                   Loss D: -0.6379, loss C: 0.5363\n",
      "Epoch [3/5] Batch 1500/3166                   Loss D: -0.7337, loss C: 0.4162\n",
      "Epoch [3/5] Batch 1600/3166                   Loss D: -0.5942, loss C: 0.2011\n",
      "Epoch [3/5] Batch 1700/3166                   Loss D: -0.6692, loss C: 0.4925\n",
      "Epoch [3/5] Batch 1800/3166                   Loss D: -0.6494, loss C: 0.2281\n",
      "Epoch [3/5] Batch 1900/3166                   Loss D: -0.5552, loss C: 0.1867\n",
      "Epoch [3/5] Batch 2000/3166                   Loss D: -0.6380, loss C: 0.5097\n",
      "Epoch [3/5] Batch 2100/3166                   Loss D: -0.6236, loss C: 0.0686\n",
      "Epoch [3/5] Batch 2200/3166                   Loss D: -0.6358, loss C: 0.1889\n",
      "Epoch [3/5] Batch 2300/3166                   Loss D: -0.6661, loss C: 0.4659\n",
      "Epoch [3/5] Batch 2400/3166                   Loss D: -0.6166, loss C: 0.2089\n",
      "Epoch [3/5] Batch 2500/3166                   Loss D: -0.5336, loss C: -0.0626\n",
      "Epoch [3/5] Batch 2600/3166                   Loss D: -0.6446, loss C: 0.1548\n",
      "Epoch [3/5] Batch 2700/3166                   Loss D: -0.5216, loss C: 0.5238\n",
      "Epoch [3/5] Batch 2800/3166                   Loss D: -0.5355, loss C: 0.1256\n",
      "Epoch [3/5] Batch 2900/3166                   Loss D: -0.5849, loss C: 0.1801\n",
      "Epoch [3/5] Batch 3000/3166                   Loss D: -0.5760, loss C: 0.1640\n",
      "Epoch [3/5] Batch 3100/3166                   Loss D: -0.5992, loss C: 0.5249\n",
      "Epoch [4/5] Batch 0/3166                   Loss D: -0.6312, loss C: 0.2143\n",
      "Epoch [4/5] Batch 100/3166                   Loss D: -0.5843, loss C: 0.1748\n",
      "Epoch [4/5] Batch 200/3166                   Loss D: -0.6114, loss C: 0.1940\n",
      "Epoch [4/5] Batch 300/3166                   Loss D: -0.5776, loss C: 0.1246\n",
      "Epoch [4/5] Batch 400/3166                   Loss D: -0.5962, loss C: 0.5233\n",
      "Epoch [4/5] Batch 500/3166                   Loss D: -0.6553, loss C: 0.2707\n",
      "Epoch [4/5] Batch 600/3166                   Loss D: -0.6056, loss C: 0.1226\n",
      "Epoch [4/5] Batch 700/3166                   Loss D: -0.6440, loss C: 0.2331\n",
      "Epoch [4/5] Batch 800/3166                   Loss D: -0.6011, loss C: 0.2138\n",
      "Epoch [4/5] Batch 900/3166                   Loss D: -0.5607, loss C: 0.1741\n",
      "Epoch [4/5] Batch 1000/3166                   Loss D: -0.6077, loss C: 0.5439\n",
      "Epoch [4/5] Batch 1100/3166                   Loss D: -0.5634, loss C: 0.1968\n",
      "Epoch [4/5] Batch 1200/3166                   Loss D: -0.6671, loss C: 0.5171\n",
      "Epoch [4/5] Batch 1300/3166                   Loss D: -0.7180, loss C: 0.5571\n",
      "Epoch [4/5] Batch 1400/3166                   Loss D: -0.5222, loss C: -0.0331\n",
      "Epoch [4/5] Batch 1500/3166                   Loss D: -0.4818, loss C: 0.5142\n",
      "Epoch [4/5] Batch 1600/3166                   Loss D: -0.5072, loss C: 0.2411\n",
      "Epoch [4/5] Batch 1700/3166                   Loss D: -0.5820, loss C: 0.1841\n",
      "Epoch [4/5] Batch 1800/3166                   Loss D: -0.7261, loss C: 0.4404\n",
      "Epoch [4/5] Batch 1900/3166                   Loss D: -0.4996, loss C: 0.5068\n",
      "Epoch [4/5] Batch 2000/3166                   Loss D: -0.5046, loss C: 0.4768\n",
      "Epoch [4/5] Batch 2100/3166                   Loss D: -0.5827, loss C: 0.1743\n",
      "Epoch [4/5] Batch 2200/3166                   Loss D: -0.5654, loss C: 0.1559\n",
      "Epoch [4/5] Batch 2300/3166                   Loss D: -0.6139, loss C: 0.4986\n",
      "Epoch [4/5] Batch 2400/3166                   Loss D: -0.6222, loss C: 0.5103\n",
      "Epoch [4/5] Batch 2500/3166                   Loss D: -0.5713, loss C: 0.2093\n",
      "Epoch [4/5] Batch 2600/3166                   Loss D: -0.5466, loss C: 0.1397\n",
      "Epoch [4/5] Batch 2700/3166                   Loss D: -0.6307, loss C: 0.5057\n",
      "Epoch [4/5] Batch 2800/3166                   Loss D: -0.5759, loss C: 0.4838\n",
      "Epoch [4/5] Batch 2900/3166                   Loss D: -0.5947, loss C: 0.0442\n",
      "Epoch [4/5] Batch 3000/3166                   Loss D: -0.6010, loss C: 0.1719\n",
      "Epoch [4/5] Batch 3100/3166                   Loss D: -0.6137, loss C: 0.5180\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    # Unsupervised\n",
    "    for batch_idx, (real, _) in enumerate(dataloader):\n",
    "        \n",
    "        # The real world images\n",
    "        real = real.to(device)\n",
    "        \n",
    "        #####################################################\n",
    "        # Train the Critic\n",
    "        #####################################################\n",
    "        \n",
    "        for _ in range(CRITIC_ITERATIONS):\n",
    "            critic.zero_grad()\n",
    "            # Latent noise\n",
    "            noise = torch.randn((BATCH_SIZE, Z_DIM, 1, 1)).to(device)\n",
    "            # Pass the latent vector through the generator\n",
    "            fake = generator(noise)     \n",
    "            critic_real = critic(real).view(-1)\n",
    "            critic_fake = critic(fake.detach()).view(-1)\n",
    "            ## Loss for the critic. Taking -ve because RMSProp are designed to minimize \n",
    "            ## Hence to minimize something -ve is equivalent to maximizing that expression\n",
    "            loss_critic = -(torch.mean(critic_real) - torch.mean(critic_fake))\n",
    "            loss_critic.backward()\n",
    "            optimizer_critic.step()\n",
    "            \n",
    "            for p in critic.parameters():\n",
    "                p.data.clamp_(-WEIGHT_CLIP, WEIGHT_CLIP)\n",
    "        \n",
    "        #############################\n",
    "        # Train the generator minimizing -E[critic(gen_fake)]\n",
    "        #############################\n",
    "        generator.zero_grad()\n",
    "        output = critic(fake).view(-1)\n",
    "        loss_gen = -torch.mean(output)\n",
    "        loss_gen.backward()\n",
    "        optimizer_gen.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            \n",
    "            print(\n",
    "            f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "                  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                fake = generator(fixed_noise)\n",
    "            \n",
    "                # The [:64] prints out the 4-D tensor BxCxHxW\n",
    "                img_grid_real = torchvision.utils.make_grid(\n",
    "                    real[:64], normalize = True)\n",
    "                img_grid_fake = torchvision.utils.make_grid(\n",
    "                    fake[:64], normalize = True)\n",
    "                ##########################\n",
    "                # TensorBoard Visualizations\n",
    "                ##########################\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "#                 loss_curves.add_scalar(\"generator\", {loss_gen, global_step=step)\n",
    "                loss_curves.add_scalars(\"curves\", {\n",
    "                    \"generator\":loss_gen, \"critic\":loss_critic\n",
    "                }, global_step = step)\n",
    "#                 loss_curves.add_scalar(\"discriminator\", loss_disc, global_step = step)\n",
    "                \n",
    "            step += 1 # See progression of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
