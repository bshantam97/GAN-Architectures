{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, datasets, transforms\n",
    "import torch.optim as optim \n",
    "import torch.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, channels_img, features_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Did not use BatchNorm in the last layer of the generator and the first layer of the \n",
    "        # discriminator\n",
    "        # Input: N x channels_img x 64 x 64\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Conv2d(channels_img, features_d, kernel_size = 4, stride = 2, padding = 1), #32x32\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            self._block(features_d, features_d*2, 4, 2, 1),# 16x16\n",
    "            self._block(features_d*2, features_d*4, 4, 2, 1), #8x8\n",
    "            self._block(features_d*4, features_d*8, 4, 2, 1), #4x4\n",
    "            nn.Conv2d(features_d*8, 1, kernel_size = 4, stride = 2, padding = 0), #1x1(Prediction)\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels,kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.discriminator(x)\n",
    "       \n",
    "                \n",
    "class Generator(nn.Module):\n",
    "    \n",
    "    # Here channels_img is nothing but the inputs channels \n",
    "    # and features_g is nothing but the output channels\n",
    "    def __init__(self, z_dim, channels_img, features_g):\n",
    "        \n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            # Input: N x z_dim x 1 x 1\n",
    "            self._block(z_dim, features_g*16, 4, 1, 0), # N x f_g*16(1024=64*64) x 4 x 4\n",
    "            self._block(features_g*16, features_g*8, 4, 2, 1), # f_g*16 x f_g*8 x 8 x 8\n",
    "            self._block(features_g*8, features_g*4, 4, 2, 1), # f_g*8 x f_g*4 x 16 x 16\n",
    "            self._block(features_g*4, features_g*2, 4, 2, 1), #32 x 32\n",
    "            nn.ConvTranspose2d(features_g*2, channels_img, 4, 2, 1),\n",
    "            nn.Tanh() #[-1,1]\n",
    "        )\n",
    "    \n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "    \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "            \n",
    "# def test():\n",
    "#     N, in_channels, H, W = 8, 3, 64, 64\n",
    "#     z_dim = 100\n",
    "#     X = torch.randn((N, in_channels, H, W))\n",
    "#     disc = Discriminator(in_channels,8)\n",
    "#     assert disc(X).shape == (N, 1, 1, 1) # One Value per example\n",
    "#     gen = Generator(z_dim, in_channels, 64)\n",
    "#     z = torch.randn((N, z_dim, 1, 1))\n",
    "#     assert gen(z).shape == (N, in_channels, H, W) # Ouput Generated image\n",
    "#     print(\"Success\")\n",
    "# test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The training Setup\n",
    "root = \"C:\\\\Users\\\\shant\\\\celeba\"\n",
    "LEARNING_RATE = 0.0002\n",
    "BATCH_SIZE = 128\n",
    "IMAGE_SIZE = 64\n",
    "# Image Channels in the generator output and input to the discriminator \n",
    "IMAGE_CHANNELS = 3\n",
    "\n",
    "# Latent Space Dimensions\n",
    "Z_DIM = 100\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "# feature size in the discriminator\n",
    "FEATURES_DISC = 64\n",
    "\n",
    "# feature size in the generator\n",
    "FEATURES_GEN = 64\n",
    "\n",
    "# Setup Transforms\n",
    "dataset = datasets.ImageFolder(root=root, transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.CenterCrop(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/15] Batch 0/1583                   Loss D: 1.9514, loss G: 6.7836\n",
      "Epoch [0/15] Batch 100/1583                   Loss D: 0.7051, loss G: 6.6420\n",
      "Epoch [0/15] Batch 200/1583                   Loss D: 1.3265, loss G: 1.4036\n",
      "Epoch [0/15] Batch 300/1583                   Loss D: 1.0734, loss G: 3.4006\n",
      "Epoch [0/15] Batch 400/1583                   Loss D: 1.3749, loss G: 1.1576\n",
      "Epoch [0/15] Batch 500/1583                   Loss D: 1.7056, loss G: 2.5363\n",
      "Epoch [0/15] Batch 600/1583                   Loss D: 1.0942, loss G: 3.8139\n",
      "Epoch [0/15] Batch 700/1583                   Loss D: 1.3290, loss G: 2.0629\n",
      "Epoch [0/15] Batch 800/1583                   Loss D: 1.3427, loss G: 1.4606\n",
      "Epoch [0/15] Batch 900/1583                   Loss D: 1.1687, loss G: 1.8242\n",
      "Epoch [0/15] Batch 1000/1583                   Loss D: 1.1839, loss G: 1.5506\n",
      "Epoch [0/15] Batch 1100/1583                   Loss D: 1.1228, loss G: 1.4442\n",
      "Epoch [0/15] Batch 1200/1583                   Loss D: 1.1467, loss G: 2.8867\n",
      "Epoch [0/15] Batch 1300/1583                   Loss D: 1.3892, loss G: 2.2111\n",
      "Epoch [0/15] Batch 1400/1583                   Loss D: 0.8383, loss G: 1.9953\n",
      "Epoch [0/15] Batch 1500/1583                   Loss D: 1.1047, loss G: 1.0541\n",
      "Epoch [1/15] Batch 0/1583                   Loss D: 1.1408, loss G: 1.2636\n",
      "Epoch [1/15] Batch 100/1583                   Loss D: 0.5558, loss G: 2.7018\n",
      "Epoch [1/15] Batch 200/1583                   Loss D: 0.9489, loss G: 1.6717\n",
      "Epoch [1/15] Batch 300/1583                   Loss D: 0.7565, loss G: 2.3900\n",
      "Epoch [1/15] Batch 400/1583                   Loss D: 0.8578, loss G: 1.5124\n",
      "Epoch [1/15] Batch 500/1583                   Loss D: 1.0171, loss G: 1.6632\n",
      "Epoch [1/15] Batch 600/1583                   Loss D: 1.0358, loss G: 2.2452\n",
      "Epoch [1/15] Batch 700/1583                   Loss D: 0.9766, loss G: 2.4785\n",
      "Epoch [1/15] Batch 800/1583                   Loss D: 0.9155, loss G: 2.4765\n",
      "Epoch [1/15] Batch 900/1583                   Loss D: 0.9992, loss G: 1.6404\n",
      "Epoch [1/15] Batch 1000/1583                   Loss D: 0.9211, loss G: 2.3780\n",
      "Epoch [1/15] Batch 1100/1583                   Loss D: 1.0213, loss G: 2.9508\n",
      "Epoch [1/15] Batch 1200/1583                   Loss D: 1.0722, loss G: 2.4738\n",
      "Epoch [1/15] Batch 1300/1583                   Loss D: 1.0099, loss G: 1.7593\n",
      "Epoch [1/15] Batch 1400/1583                   Loss D: 0.8995, loss G: 1.4522\n",
      "Epoch [1/15] Batch 1500/1583                   Loss D: 1.6480, loss G: 0.6815\n",
      "Epoch [2/15] Batch 0/1583                   Loss D: 1.1472, loss G: 1.9221\n",
      "Epoch [2/15] Batch 100/1583                   Loss D: 0.9006, loss G: 2.0637\n",
      "Epoch [2/15] Batch 200/1583                   Loss D: 1.1615, loss G: 2.3494\n",
      "Epoch [2/15] Batch 300/1583                   Loss D: 0.9024, loss G: 1.4799\n",
      "Epoch [2/15] Batch 400/1583                   Loss D: 1.0417, loss G: 2.2698\n",
      "Epoch [2/15] Batch 500/1583                   Loss D: 1.3373, loss G: 0.7455\n",
      "Epoch [2/15] Batch 600/1583                   Loss D: 1.0912, loss G: 1.2918\n",
      "Epoch [2/15] Batch 700/1583                   Loss D: 1.0133, loss G: 1.1358\n",
      "Epoch [2/15] Batch 800/1583                   Loss D: 0.9849, loss G: 2.0100\n",
      "Epoch [2/15] Batch 900/1583                   Loss D: 1.4636, loss G: 0.9929\n",
      "Epoch [2/15] Batch 1000/1583                   Loss D: 1.0619, loss G: 2.3175\n",
      "Epoch [2/15] Batch 1100/1583                   Loss D: 1.0384, loss G: 1.2020\n",
      "Epoch [2/15] Batch 1200/1583                   Loss D: 1.0109, loss G: 1.0709\n",
      "Epoch [2/15] Batch 1300/1583                   Loss D: 0.9244, loss G: 1.7036\n",
      "Epoch [2/15] Batch 1400/1583                   Loss D: 1.4136, loss G: 0.7898\n",
      "Epoch [2/15] Batch 1500/1583                   Loss D: 1.0452, loss G: 1.5229\n",
      "Epoch [3/15] Batch 0/1583                   Loss D: 1.2351, loss G: 1.2644\n",
      "Epoch [3/15] Batch 100/1583                   Loss D: 0.9698, loss G: 1.4978\n",
      "Epoch [3/15] Batch 200/1583                   Loss D: 1.2933, loss G: 2.0898\n",
      "Epoch [3/15] Batch 300/1583                   Loss D: 1.0916, loss G: 1.1770\n",
      "Epoch [3/15] Batch 400/1583                   Loss D: 1.1201, loss G: 1.3939\n",
      "Epoch [3/15] Batch 500/1583                   Loss D: 1.0094, loss G: 1.9017\n",
      "Epoch [3/15] Batch 600/1583                   Loss D: 1.2866, loss G: 0.7005\n",
      "Epoch [3/15] Batch 700/1583                   Loss D: 1.0293, loss G: 0.9815\n",
      "Epoch [3/15] Batch 800/1583                   Loss D: 1.1622, loss G: 0.9516\n",
      "Epoch [3/15] Batch 900/1583                   Loss D: 1.0273, loss G: 1.9474\n",
      "Epoch [3/15] Batch 1000/1583                   Loss D: 1.3365, loss G: 1.8990\n",
      "Epoch [3/15] Batch 1100/1583                   Loss D: 1.1831, loss G: 1.1687\n",
      "Epoch [3/15] Batch 1200/1583                   Loss D: 1.1184, loss G: 2.1731\n",
      "Epoch [3/15] Batch 1300/1583                   Loss D: 1.6502, loss G: 0.3757\n",
      "Epoch [3/15] Batch 1400/1583                   Loss D: 0.9923, loss G: 1.6601\n",
      "Epoch [3/15] Batch 1500/1583                   Loss D: 1.3521, loss G: 0.4195\n",
      "Epoch [4/15] Batch 0/1583                   Loss D: 1.0851, loss G: 1.4798\n",
      "Epoch [4/15] Batch 100/1583                   Loss D: 1.0451, loss G: 1.7415\n",
      "Epoch [4/15] Batch 200/1583                   Loss D: 0.8900, loss G: 1.4910\n",
      "Epoch [4/15] Batch 300/1583                   Loss D: 1.0039, loss G: 1.7866\n",
      "Epoch [4/15] Batch 400/1583                   Loss D: 1.2730, loss G: 0.7104\n",
      "Epoch [4/15] Batch 500/1583                   Loss D: 1.0690, loss G: 1.3727\n",
      "Epoch [4/15] Batch 600/1583                   Loss D: 1.1319, loss G: 1.9844\n",
      "Epoch [4/15] Batch 700/1583                   Loss D: 1.1213, loss G: 2.0749\n",
      "Epoch [4/15] Batch 800/1583                   Loss D: 1.1036, loss G: 2.0273\n",
      "Epoch [4/15] Batch 900/1583                   Loss D: 1.0958, loss G: 1.7125\n",
      "Epoch [4/15] Batch 1000/1583                   Loss D: 1.3947, loss G: 2.2021\n",
      "Epoch [4/15] Batch 1100/1583                   Loss D: 1.2176, loss G: 1.4806\n",
      "Epoch [4/15] Batch 1200/1583                   Loss D: 1.2242, loss G: 1.9981\n",
      "Epoch [4/15] Batch 1300/1583                   Loss D: 1.1133, loss G: 0.8291\n",
      "Epoch [4/15] Batch 1400/1583                   Loss D: 1.2191, loss G: 0.8327\n",
      "Epoch [4/15] Batch 1500/1583                   Loss D: 1.1112, loss G: 1.0758\n",
      "Epoch [5/15] Batch 0/1583                   Loss D: 1.0112, loss G: 1.5924\n",
      "Epoch [5/15] Batch 100/1583                   Loss D: 0.9916, loss G: 1.1552\n",
      "Epoch [5/15] Batch 200/1583                   Loss D: 1.0488, loss G: 1.2419\n",
      "Epoch [5/15] Batch 300/1583                   Loss D: 1.1700, loss G: 1.9723\n",
      "Epoch [5/15] Batch 400/1583                   Loss D: 1.0869, loss G: 1.0734\n",
      "Epoch [5/15] Batch 500/1583                   Loss D: 0.8853, loss G: 1.9530\n",
      "Epoch [5/15] Batch 600/1583                   Loss D: 0.9655, loss G: 1.6427\n",
      "Epoch [5/15] Batch 700/1583                   Loss D: 1.2524, loss G: 2.0733\n",
      "Epoch [5/15] Batch 800/1583                   Loss D: 0.9973, loss G: 1.3876\n",
      "Epoch [5/15] Batch 900/1583                   Loss D: 0.9765, loss G: 1.7307\n",
      "Epoch [5/15] Batch 1000/1583                   Loss D: 1.0635, loss G: 1.7215\n",
      "Epoch [5/15] Batch 1100/1583                   Loss D: 1.2387, loss G: 1.9840\n",
      "Epoch [5/15] Batch 1200/1583                   Loss D: 0.9853, loss G: 1.5929\n",
      "Epoch [5/15] Batch 1300/1583                   Loss D: 0.9263, loss G: 2.0639\n",
      "Epoch [5/15] Batch 1400/1583                   Loss D: 1.1893, loss G: 1.0271\n",
      "Epoch [5/15] Batch 1500/1583                   Loss D: 1.0417, loss G: 1.3344\n",
      "Epoch [6/15] Batch 0/1583                   Loss D: 1.0063, loss G: 1.2427\n",
      "Epoch [6/15] Batch 100/1583                   Loss D: 1.0679, loss G: 1.1702\n",
      "Epoch [6/15] Batch 200/1583                   Loss D: 0.9865, loss G: 1.7290\n",
      "Epoch [6/15] Batch 300/1583                   Loss D: 1.1506, loss G: 2.0734\n",
      "Epoch [6/15] Batch 400/1583                   Loss D: 1.1166, loss G: 0.9519\n",
      "Epoch [6/15] Batch 500/1583                   Loss D: 0.8919, loss G: 1.4273\n",
      "Epoch [6/15] Batch 600/1583                   Loss D: 1.0412, loss G: 1.1255\n",
      "Epoch [6/15] Batch 700/1583                   Loss D: 0.7973, loss G: 1.7014\n",
      "Epoch [6/15] Batch 800/1583                   Loss D: 0.8619, loss G: 1.8824\n",
      "Epoch [6/15] Batch 900/1583                   Loss D: 1.1814, loss G: 0.3824\n",
      "Epoch [6/15] Batch 1000/1583                   Loss D: 1.6390, loss G: 3.1599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/15] Batch 1100/1583                   Loss D: 0.8182, loss G: 1.8134\n",
      "Epoch [6/15] Batch 1200/1583                   Loss D: 0.8298, loss G: 1.9510\n",
      "Epoch [6/15] Batch 1300/1583                   Loss D: 1.1131, loss G: 1.2655\n",
      "Epoch [6/15] Batch 1400/1583                   Loss D: 0.9444, loss G: 1.4348\n",
      "Epoch [6/15] Batch 1500/1583                   Loss D: 1.0246, loss G: 1.2548\n",
      "Epoch [7/15] Batch 0/1583                   Loss D: 0.8370, loss G: 1.8794\n",
      "Epoch [7/15] Batch 100/1583                   Loss D: 0.9376, loss G: 2.1625\n",
      "Epoch [7/15] Batch 200/1583                   Loss D: 1.0392, loss G: 0.8447\n",
      "Epoch [7/15] Batch 300/1583                   Loss D: 0.9532, loss G: 2.5218\n",
      "Epoch [7/15] Batch 400/1583                   Loss D: 0.9208, loss G: 1.2008\n",
      "Epoch [7/15] Batch 500/1583                   Loss D: 0.8692, loss G: 1.3401\n",
      "Epoch [7/15] Batch 600/1583                   Loss D: 0.8806, loss G: 1.2222\n",
      "Epoch [7/15] Batch 700/1583                   Loss D: 1.1149, loss G: 0.6952\n",
      "Epoch [7/15] Batch 800/1583                   Loss D: 1.0323, loss G: 2.3860\n",
      "Epoch [7/15] Batch 900/1583                   Loss D: 0.9005, loss G: 2.1789\n",
      "Epoch [7/15] Batch 1000/1583                   Loss D: 0.7796, loss G: 2.0610\n",
      "Epoch [7/15] Batch 1100/1583                   Loss D: 0.8863, loss G: 1.3335\n",
      "Epoch [7/15] Batch 1200/1583                   Loss D: 0.8594, loss G: 1.0441\n",
      "Epoch [7/15] Batch 1300/1583                   Loss D: 0.9702, loss G: 1.3344\n",
      "Epoch [7/15] Batch 1400/1583                   Loss D: 0.8081, loss G: 1.2515\n",
      "Epoch [7/15] Batch 1500/1583                   Loss D: 1.9248, loss G: 3.7957\n",
      "Epoch [8/15] Batch 0/1583                   Loss D: 0.7796, loss G: 1.6714\n",
      "Epoch [8/15] Batch 100/1583                   Loss D: 1.0718, loss G: 1.0492\n",
      "Epoch [8/15] Batch 200/1583                   Loss D: 0.9091, loss G: 1.3129\n",
      "Epoch [8/15] Batch 300/1583                   Loss D: 1.0536, loss G: 0.9017\n",
      "Epoch [8/15] Batch 400/1583                   Loss D: 1.1252, loss G: 1.4448\n",
      "Epoch [8/15] Batch 500/1583                   Loss D: 0.7421, loss G: 1.4873\n",
      "Epoch [8/15] Batch 600/1583                   Loss D: 1.2265, loss G: 2.6528\n",
      "Epoch [8/15] Batch 700/1583                   Loss D: 0.8036, loss G: 1.7110\n",
      "Epoch [8/15] Batch 800/1583                   Loss D: 0.8323, loss G: 1.4968\n",
      "Epoch [8/15] Batch 900/1583                   Loss D: 0.8995, loss G: 2.4118\n",
      "Epoch [8/15] Batch 1000/1583                   Loss D: 0.8754, loss G: 2.2998\n",
      "Epoch [8/15] Batch 1100/1583                   Loss D: 0.9438, loss G: 1.1434\n",
      "Epoch [8/15] Batch 1200/1583                   Loss D: 0.9729, loss G: 2.6232\n",
      "Epoch [8/15] Batch 1300/1583                   Loss D: 0.9457, loss G: 1.6859\n",
      "Epoch [8/15] Batch 1400/1583                   Loss D: 1.0235, loss G: 1.1155\n",
      "Epoch [8/15] Batch 1500/1583                   Loss D: 0.8120, loss G: 1.4350\n",
      "Epoch [9/15] Batch 0/1583                   Loss D: 0.7474, loss G: 2.5234\n",
      "Epoch [9/15] Batch 100/1583                   Loss D: 1.2210, loss G: 1.9027\n",
      "Epoch [9/15] Batch 200/1583                   Loss D: 1.0103, loss G: 2.0841\n",
      "Epoch [9/15] Batch 300/1583                   Loss D: 0.7731, loss G: 1.7702\n",
      "Epoch [9/15] Batch 400/1583                   Loss D: 1.5208, loss G: 3.0228\n",
      "Epoch [9/15] Batch 500/1583                   Loss D: 0.8552, loss G: 1.2267\n",
      "Epoch [9/15] Batch 600/1583                   Loss D: 1.1142, loss G: 3.1444\n",
      "Epoch [9/15] Batch 700/1583                   Loss D: 0.8379, loss G: 1.9068\n",
      "Epoch [9/15] Batch 800/1583                   Loss D: 0.8121, loss G: 1.3769\n",
      "Epoch [9/15] Batch 900/1583                   Loss D: 0.7685, loss G: 2.1192\n",
      "Epoch [9/15] Batch 1000/1583                   Loss D: 0.9743, loss G: 0.9924\n",
      "Epoch [9/15] Batch 1100/1583                   Loss D: 1.0720, loss G: 3.9383\n",
      "Epoch [9/15] Batch 1200/1583                   Loss D: 0.9578, loss G: 0.9667\n",
      "Epoch [9/15] Batch 1300/1583                   Loss D: 1.0639, loss G: 1.7395\n",
      "Epoch [9/15] Batch 1400/1583                   Loss D: 0.8864, loss G: 1.5622\n",
      "Epoch [9/15] Batch 1500/1583                   Loss D: 0.7409, loss G: 1.3541\n",
      "Epoch [10/15] Batch 0/1583                   Loss D: 0.8842, loss G: 1.6935\n",
      "Epoch [10/15] Batch 100/1583                   Loss D: 0.7585, loss G: 2.3453\n",
      "Epoch [10/15] Batch 200/1583                   Loss D: 0.9040, loss G: 1.2870\n",
      "Epoch [10/15] Batch 300/1583                   Loss D: 0.6765, loss G: 1.5356\n",
      "Epoch [10/15] Batch 400/1583                   Loss D: 0.8221, loss G: 1.3845\n",
      "Epoch [10/15] Batch 500/1583                   Loss D: 0.6624, loss G: 2.0354\n",
      "Epoch [10/15] Batch 600/1583                   Loss D: 0.4842, loss G: 2.5462\n",
      "Epoch [10/15] Batch 700/1583                   Loss D: 0.5786, loss G: 2.0840\n",
      "Epoch [10/15] Batch 800/1583                   Loss D: 0.7491, loss G: 1.9153\n",
      "Epoch [10/15] Batch 900/1583                   Loss D: 0.7919, loss G: 2.0194\n",
      "Epoch [10/15] Batch 1000/1583                   Loss D: 0.7968, loss G: 3.0962\n",
      "Epoch [10/15] Batch 1100/1583                   Loss D: 0.8713, loss G: 2.6147\n",
      "Epoch [10/15] Batch 1200/1583                   Loss D: 0.6882, loss G: 1.4674\n",
      "Epoch [10/15] Batch 1300/1583                   Loss D: 0.6055, loss G: 1.7298\n",
      "Epoch [10/15] Batch 1400/1583                   Loss D: 0.6779, loss G: 1.9950\n",
      "Epoch [10/15] Batch 1500/1583                   Loss D: 0.5810, loss G: 2.4253\n",
      "Epoch [11/15] Batch 0/1583                   Loss D: 0.6625, loss G: 2.8222\n",
      "Epoch [11/15] Batch 100/1583                   Loss D: 0.5719, loss G: 2.2546\n",
      "Epoch [11/15] Batch 200/1583                   Loss D: 0.7145, loss G: 2.6632\n",
      "Epoch [11/15] Batch 300/1583                   Loss D: 0.5937, loss G: 2.1278\n",
      "Epoch [11/15] Batch 400/1583                   Loss D: 0.9982, loss G: 1.1515\n",
      "Epoch [11/15] Batch 500/1583                   Loss D: 1.4290, loss G: 4.7370\n",
      "Epoch [11/15] Batch 600/1583                   Loss D: 0.7494, loss G: 2.2180\n",
      "Epoch [11/15] Batch 700/1583                   Loss D: 0.6906, loss G: 1.6784\n",
      "Epoch [11/15] Batch 800/1583                   Loss D: 0.6654, loss G: 1.8095\n",
      "Epoch [11/15] Batch 900/1583                   Loss D: 0.7070, loss G: 2.1176\n",
      "Epoch [11/15] Batch 1000/1583                   Loss D: 0.6829, loss G: 2.3131\n",
      "Epoch [11/15] Batch 1100/1583                   Loss D: 0.9967, loss G: 1.0283\n",
      "Epoch [11/15] Batch 1200/1583                   Loss D: 1.0470, loss G: 0.9104\n",
      "Epoch [11/15] Batch 1300/1583                   Loss D: 0.7133, loss G: 3.2639\n",
      "Epoch [11/15] Batch 1400/1583                   Loss D: 0.6103, loss G: 1.8543\n",
      "Epoch [11/15] Batch 1500/1583                   Loss D: 0.5922, loss G: 1.9702\n"
     ]
    }
   ],
   "source": [
    "# DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size = BATCH_SIZE, shuffle=True, num_workers = 2)\n",
    "\n",
    "device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = Generator(Z_DIM, IMAGE_CHANNELS, FEATURES_GEN).to(device)\n",
    "discriminator = Discriminator(IMAGE_CHANNELS,FEATURES_DISC).to(device)\n",
    "\n",
    "generator.apply(weights_init)\n",
    "discriminator.apply(weights_init)\n",
    "\n",
    "generator_optimizer = optim.Adam(generator.parameters(), lr = LEARNING_RATE, betas = (0.5,0.999))\n",
    "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr = LEARNING_RATE, betas = (0.5,0.999))\n",
    "\n",
    "# Loss \n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Batch of Latent Vectors\n",
    "fixed_noise = torch.randn((64, Z_DIM, 1, 1)).to(device)\n",
    "\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "loss_curves = SummaryWriter(f\"logs/loss_curves\")\n",
    "\n",
    "step = 0 # Printing to tensorboard\n",
    "# generator.train()\n",
    "# discriminator.train()\n",
    "\n",
    "generator_losses = []\n",
    "discriminator_losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    # Unsupervised\n",
    "    for batch_idx, (real, _) in enumerate(dataloader):\n",
    "        discriminator.zero_grad()\n",
    "        # Latent noise\n",
    "        noise = torch.randn((BATCH_SIZE, Z_DIM, 1, 1)).to(device)\n",
    "        # The real world images\n",
    "        real = real.to(device)\n",
    "        # Pass the latent vector through the generator\n",
    "        fake = generator(noise)\n",
    "        #####################################################\n",
    "        # Train the discriminator max log(D(x)) + log(1-D(G(z)))\n",
    "        #####################################################\n",
    "        \n",
    "        disc_real = discriminator(real).view(-1)\n",
    "        # log(D(x)), y_n = ones hence only logx_n term is left. Refer to Pytorch Documentation\n",
    "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        \n",
    "        loss_disc_real.backward()\n",
    "        \n",
    "        disc_fake = discriminator(fake.detach()).view(-1)\n",
    "        # Subtracting \n",
    "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        \n",
    "        # Addition of gradients from the all the real and fake samples (D(G(z)))\n",
    "        loss_disc = (loss_disc_fake + loss_disc_real)\n",
    "        \n",
    "        loss_disc_fake.backward()\n",
    "        discriminator_optimizer.step()\n",
    "        \n",
    "        ##################################\n",
    "        # Train Generator max log(D(G(z)))\n",
    "        ##################################\n",
    "        generator.zero_grad()\n",
    "        output = discriminator(fake).view(-1)\n",
    "        loss_gen = criterion(output, torch.ones_like(output))\n",
    "        loss_gen.backward()\n",
    "        generator_optimizer.step()\n",
    "        \n",
    "        #generator_losses.append(loss_gen.item())\n",
    "        discriminator_losses.append(loss_disc)\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            \n",
    "            print(\n",
    "            f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                fake = generator(fixed_noise)\n",
    "            \n",
    "                # The [:64] prints out the 4-D tensor BxCxHxW\n",
    "                img_grid_real = torchvision.utils.make_grid(\n",
    "                    real[:64], normalize = True)\n",
    "                img_grid_fake = torchvision.utils.make_grid(\n",
    "                    fake[:64], normalize = True)\n",
    "                ##########################\n",
    "                # TensorBoard Visualizations\n",
    "                ##########################\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "                loss_curves.add_scalar(\"generator\", loss_gen, global_step=step)\n",
    "                loss_curves.add_scalar(\"discriminator\", loss_disc, global_step = step)\n",
    "                \n",
    "            step += 1 # See progression of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
